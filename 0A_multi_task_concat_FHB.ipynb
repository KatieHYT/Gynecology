{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load 0A_multi_task_concat_FHB.py\n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import keras\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "# get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from keras.optimizers import Adam, SGD, Adamax\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from func import * \n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "#### parser\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-s' ,'--model_save', type=str, default='/home/katieyth/gynecology/model_save/', help='model save path')\n",
    "parser.add_argument('-y' ,'--target', type=str, default='multi', help='prediction target')\n",
    "parser.add_argument('-cv_path' ,'--cv_path', type=str, default='/home/katieyth/gynecology/data/5_fold_CMU_rs_13/', help='cv_path')\n",
    "\n",
    " \n",
    "# input parameter\n",
    "parser.add_argument('-th','--acceptable_zeros_threshold', type=float, default=200, help='acceptable number of missing values in raw data')\n",
    "parser.add_argument('-l' ,'--length', type=int, default=600, help='length of input')\n",
    "parser.add_argument('-ks','--k_slice', type=int, default=1, help='a input will be sliced into k_slice segments when testing')\n",
    "parser.add_argument('-c' ,'--n_channel', type=int, default=2, help='number of input channels')\n",
    "parser.add_argument('-rn','--random_noise', type=int, default=0, help='add Gaussian noise (mean=0, std=0.01) into inputs')\n",
    "parser.add_argument('-nm','--normalized', type=int, default=1, help='whether conduct channel-wise normalization')\n",
    "\n",
    "# data augmentation \n",
    "parser.add_argument('-aug_fliplr' ,'--aug_fliplr', type=int, default=0, help='reverse time series')\n",
    "parser.add_argument('-shift' ,'--DA_Shift', type=int, default=1, help='')\n",
    "parser.add_argument('-scale' ,'--DA_Scale', type=int, default=1, help='')\n",
    "parser.add_argument('-randsamp' ,'--DA_RandSampling', type=int, default=1, help='')\n",
    "\n",
    "\n",
    "# model parameters\n",
    "parser.add_argument('-struc' ,'--struc', type=str, default='mimic_previous_FHB', help='deeper or shallower')\n",
    "parser.add_argument('-k' ,'--kernel_size', type=int, default=3, help='kernel size')\n",
    "parser.add_argument('-f' ,'--filters', type=int, default=64, help='base number of filters')\n",
    "parser.add_argument('-ly' ,'--layers', type=int, default=5, help='number of residual layers')\n",
    "parser.add_argument('-a' ,'--activation', type=str, default='relu', help='activation function')\n",
    "parser.add_argument('-i' ,'--kernel_initializer', type=str, default='RandomNormal', help='kernel initialization method')\n",
    "parser.add_argument('-l2','--l2', type=float, default=0.01, help='coefficient of l2 regularization')\n",
    "\n",
    "# hyper-parameters\n",
    "parser.add_argument('-lr','--learning_rate', type=float, default=1e-4, help='learning_rate')\n",
    "parser.add_argument('-reduce_lr_patience','--reduce_lr_patience', type=int, default=50, help='reduce_lr_patience')\n",
    "parser.add_argument('-bs','--batch_size', type=int, default=27, help='batch_size')\n",
    "parser.add_argument('-ep','--epoch', type=int, default=15, help='epoch')\n",
    "parser.add_argument('-wb','--weight_balance', type=int, default=1, help='whether weight balancing or not')\n",
    "parser.add_argument('-mntr','--monitor', type=str, default='val_man_acc', help='val_acc or val_loss')\n",
    "\n",
    "parser.add_argument('-g' ,'--gpu_id', type=str, default='6', help='GPU ID')\n",
    "parser.add_argument('-fd' ,'--fold', type=int, default=1, help='CMU_5_fold')\n",
    "parser.add_argument('-fn' ,'--summary_file', type=str, default='', help='summary filename')\n",
    "\n",
    "FLAG = parser.parse_args()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = FLAG.gpu_id\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def data_preprocess_test(Xvalid, Yvalid, length=600):\n",
    "    Xtest = np.empty((Xvalid.shape[0], length, Xvalid.shape[2]))\n",
    "    for i in range(Xvalid.shape[0]):\n",
    "        Xtest[i,:,:] = data_normalize(Xvalid[i,0:600,:])\n",
    "    Ytest = Yvalid.copy()\n",
    "    return Xtest, Ytest\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "### data preparing\n",
    "train_d = pd.read_csv(os.path.join(FLAG.cv_path, '5_fold_0%s_train.csv' %(FLAG.fold)))\n",
    "valid_d = pd.read_csv(os.path.join(FLAG.cv_path, '5_fold_0%s_test.csv' %(FLAG.fold)))\n",
    "\n",
    "# replace 0 (no readings) with np.nan for later substitution\n",
    "for k in train_d.columns:\n",
    "    if 'b-' in k or 'm-' in k:\n",
    "        print(k, end='\\r')\n",
    "        train_d.loc[train_d[k]==0, k] = np.nan\n",
    "for k in valid_d.columns:\n",
    "    if 'b-' in k or 'm-' in k:\n",
    "        print(k, end='\\r')\n",
    "        valid_d.loc[valid_d[k]==0, k] = np.nan\n",
    "\n",
    "    \n",
    "# interpolate missing values\n",
    "train_db = np.array(train_d[[k for k in train_d.columns if 'b-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "train_dm = np.array(train_d[[k for k in train_d.columns if 'm-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "valid_db = np.array(valid_d[[k for k in valid_d.columns if 'b-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "valid_dm = np.array(valid_d[[k for k in valid_d.columns if 'm-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "# combine signals from baby and mom\n",
    "Xtrain = np.stack([train_db, train_dm], axis=2)\n",
    "Xvalid = np.stack([valid_db, valid_dm], axis=2)\n",
    "\n",
    "# convert labels to one-hot encodings\n",
    "target_list = ['management', 'UA', 'variability', 'deceleration', 'FHB_class', 'management']\n",
    "target_dict = {'management':3,\n",
    "               'UA':2,\n",
    "               'variability':2,\n",
    "               'deceleration':4,\n",
    "               'FHB_class':4,\n",
    "               'management':3}\n",
    "Ytrain = [(keras.utils.to_categorical(np.array(train_d[c_name]),num_classes=target_dict[c_name])) for c_name in target_list]\n",
    "Yvalid = [(keras.utils.to_categorical(np.array(valid_d[c_name]),num_classes=target_dict[c_name])) for c_name in target_list]\n",
    "\n",
    "#weight balancing or not\n",
    "if FLAG.weight_balance:\n",
    "    weight_list = []\n",
    "    for idx, c_name in enumerate(target_list):\n",
    "        y_integers = np.argmax(Ytrain[idx], axis=1)\n",
    "        d_class_weight = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "        class_weight = dict(enumerate(d_class_weight))\n",
    "        print('class weight: {0}'.format(class_weight))\n",
    "        weight_list.append(class_weight)\n",
    "else:\n",
    "    weight_list = []\n",
    "    for idx, c_name in enumerate(target_list):\n",
    "        class_weight = dict()\n",
    "        for i in range(target_dict[c_name]):\n",
    "            class_weight[i] = 1\n",
    "        print('class weight: {0}'.format(class_weight))\n",
    "        weight_list.append(class_weight)\n",
    "\n",
    "Xtest, Ytest =data_preprocess_test(Xvalid, Yvalid)\n",
    "\n",
    "\n",
    "if FLAG.aug_fliplr:\n",
    "    Xtrain_copy = Xtrain.copy()\n",
    "    for i in range(len(Xtrain)):\n",
    "        Xtrain_copy[i] = np.fliplr([Xtrain[i]])[0]\n",
    "        print(i,'/',len(Xtrain), end= '\\r')\n",
    "    Xtrain = np.vstack((Xtrain, Xtrain_copy))\n",
    "    Ytrain = np.vstack((Ytrain, Ytrain))\n",
    "\n",
    "print('train:', len(train_d))\n",
    "print('test:', len(valid_d))\n",
    "\n",
    "\n",
    "# # model\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "if FLAG.struc =='deeper':\n",
    "    from model_multitask_more import build_model\n",
    "elif FLAG.struc =='shallower':\n",
    "    from model_multitask import build_model\n",
    "elif FLAG.struc =='man_concat':\n",
    "    from model_multitask_more_concat import build_model\n",
    "elif FLAG.struc =='man_concat_shallower':\n",
    "    from model_multitask_more_concat_shallower import build_model\n",
    "elif FLAG.struc =='mimic_previous':\n",
    "    from model_multitask_more_concat_mimic_previous import build_model\n",
    "elif FLAG.struc =='mimic_previous_FHB':\n",
    "    from model_multitask_more_concat_mimic_previous_FHB import build_model\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "augtool = (DA_Shift,DA_Scale,DA_RandSampling)\n",
    "choose_augtool = (FLAG.DA_Shift,FLAG.DA_Scale,FLAG.DA_RandSampling)\n",
    "augset = [x for x, y in zip(augtool, choose_augtool) if y == 1]\n",
    "augset\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def my_generator(Xtrain, Ytrain, tg_list, tg_dict,\n",
    "                 length=300, n_channel=2,  batch_size=16, prob=0.5,\n",
    "                 aug_func=[], random_noise = False, normalized = True):\n",
    "    n_sample = Xtrain.shape[0]\n",
    "    n_length = Xtrain.shape[1]\n",
    "    ind = list(range(n_sample))\n",
    "    x = np.empty((batch_size, length, n_channel), dtype=np.float)\n",
    "    \n",
    "    y_list = [(np.empty((batch_size, tg_dict[c_name]), dtype=int)) for c_name in tg_list]\n",
    "\n",
    "    while True:\n",
    "        np.random.shuffle(ind)\n",
    "        for i in range(n_sample//batch_size):\n",
    "            if length==600:\n",
    "                st = 0\n",
    "            else:\n",
    "                st = random.choice(np.arange(0, Xtrain.shape[1] - length))\n",
    "            i_batch = ind[i*batch_size:(i+1)*batch_size]\n",
    "            for j, k in enumerate(i_batch):\n",
    "                x[j,:] = data_preprocess(Xtrain[k,st:(st+length),:], aug_func=aug_func, prob=prob, random_noise=random_noise, normalized=normalized)\n",
    "                for c_idx in range(len(tg_list)):\n",
    "                    y_list[c_idx][j,:] = Ytrain[c_idx][k,:] \n",
    "                \n",
    "            yield x, y_list\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "train_gen = my_generator(Xtrain, Ytrain, \n",
    "                         tg_list = target_list,\n",
    "                         tg_dict = target_dict,\n",
    "                        length=FLAG.length, \n",
    "                        n_channel=FLAG.n_channel, \n",
    "                        random_noise=FLAG.random_noise,\n",
    "                        normalized=FLAG.normalized,\n",
    "                        batch_size=FLAG.batch_size,\n",
    "                        aug_func=augset,\n",
    "                        prob=0.25)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "# Get a \"l2 norm of gradients\" tensor\n",
    "def get_gradient_norm(model):\n",
    "    with K.name_scope('gradient_norm'):\n",
    "        grads = K.gradients(model.total_loss, model.trainable_weights)\n",
    "        norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))\n",
    "    return norm\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "print(\"===== create directory =====\")\n",
    "model_id = FLAG.target + \"_\" + datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "model_save = os.path.join(FLAG.model_save, FLAG.target, model_id)\n",
    "summary_save = os.path.join(FLAG.model_save, FLAG.target, 'summary_'+FLAG.target+'_'+FLAG.summary_file+'.csv')\n",
    "\n",
    "if not os.path.exists(model_save):\n",
    "    os.makedirs(model_save)\n",
    "    print(model_save)\n",
    "\n",
    "# declare model and compile it \n",
    "model = build_model(length=FLAG.length, n_channel=FLAG.n_channel,  filters=FLAG.filters,\n",
    "                    kernel_size=FLAG.kernel_size, layers=FLAG.layers,\n",
    "                activation=FLAG.activation, kernel_initializer=FLAG.kernel_initializer, l_2=FLAG.l2)\n",
    "model.summary()\n",
    "lr_rate = FLAG.learning_rate\n",
    "adam = Adamax(lr_rate, beta_1=0.5, beta_2=0.999, epsilon=1e-08, decay = 0.0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 還沒有 compile 前拿不到 total loss 這個 tensor, 也拿不到 metrics 這個東西\n",
    "# Append the \"l2 norm of gradients\" tensor as a metric\n",
    "model.metrics_names.append(\"gradient_norm\")\n",
    "model.metrics_tensors.append(get_gradient_norm(model))\n",
    "\n",
    "\n",
    "csv_logger = CSVLogger(os.path.join(model_save, 'training.log'), append = True)\n",
    "checkpoint = ModelCheckpoint(os.path.join(model_save, 'model.h5'), \n",
    "                                            monitor=FLAG.monitor, \n",
    "                                            verbose=1, \n",
    "                                            save_best_only=True,\n",
    "                                            save_weights_only=False,\n",
    "                                            mode='auto',\n",
    "                                            period=1)\n",
    "earlystop = EarlyStopping(monitor = FLAG.monitor, patience=200, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=FLAG.monitor, factor = 0.5, patience = FLAG.reduce_lr_patience, min_lr = 0, cooldown = 5, verbose = True)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "model.fit_generator(generator=train_gen,\n",
    "                    class_weight={'man': weight_list[0],\n",
    "                                  'ua' : weight_list[1],\n",
    "                                  'var': weight_list[2],\n",
    "                                  'dec': weight_list[3],\n",
    "                                  'fhb': weight_list[4],\n",
    "                                  'man_concat':weight_list[5]},\n",
    "                    validation_data=(Xtest, {'man':Ytest[0],\n",
    "                                             'ua' :Ytest[1],\n",
    "                                             'var':Ytest[2],\n",
    "                                             'dec':Ytest[3],\n",
    "                                             'fhb':Ytest[4],\n",
    "                                             'man_concat':Ytest[5]}),\n",
    "                    steps_per_epoch=50, \n",
    "                    epochs=FLAG.epoch,\n",
    "                    verbose=1,\n",
    "                    callbacks=[csv_logger,\n",
    "                               reduce_lr, \n",
    "                               checkpoint,\n",
    "                               earlystop\n",
    "                              ])\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "def plot_keras_csv_logger(csv_logger, save_dir='', accuracy=False, gradient=False):\n",
    "    if type(csv_logger) is str:\n",
    "        loss = pd.read_table(csv_logger, delimiter=',')\n",
    "    else:\n",
    "        loss = pd.read_table(csv_logger.filename, delimiter=',')\n",
    "    print('min val_loss {0} at epoch {1}'.format(min(loss.val_loss), np.argmin(loss.val_loss)))\n",
    "    plt.plot(loss.epoch, loss.loss, label='loss')\n",
    "    plt.plot(loss.epoch, loss.val_loss, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.savefig(os.path.join(save_dir, 'loss.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    if accuracy:\n",
    "        print('max val_accu {0} at epoch {1}'.format(max(loss.val_dec_acc), np.argmax(loss.val_dec_acc)))\n",
    "        plt.plot(loss.epoch, loss.dec_acc, label='dec_acc')\n",
    "        plt.plot(loss.epoch, loss.val_dec_acc, label='val_dec_acc')\n",
    "        plt.legend()\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.savefig(os.path.join(save_dir, 'dec_acc.png'))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        print('max val_accu {0} at epoch {1}'.format(max(loss.val_var_acc), np.argmax(loss.val_var_acc)))\n",
    "        plt.plot(loss.epoch, loss.var_acc, label='var_acc')\n",
    "        plt.plot(loss.epoch, loss.val_var_acc, label='val_var_acc')\n",
    "        plt.legend()\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.savefig(os.path.join(save_dir, 'var_acc.png'))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        print('max val_accu {0} at epoch {1}'.format(max(loss.val_ua_acc), np.argmax(loss.val_ua_acc)))\n",
    "        plt.plot(loss.epoch, loss.ua_acc, label='ua_acc')\n",
    "        plt.plot(loss.epoch, loss.val_ua_acc, label='val_ua_acc')\n",
    "        plt.legend()\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.savefig(os.path.join(save_dir, 'ua_acc.png'))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        print('max val_accu {0} at epoch {1}'.format(max(loss.val_man_acc), np.argmax(loss.val_man_acc)))\n",
    "        plt.plot(loss.epoch, loss.man_acc, label='man_acc')\n",
    "        plt.plot(loss.epoch, loss.val_man_acc, label='val_man_acc')\n",
    "        plt.legend()\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.savefig(os.path.join(save_dir, 'man_acc.png'))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        print('max val_accu {0} at epoch {1}'.format(max(loss.val_man_concat_acc), np.argmax(loss.val_man_concat_acc)))\n",
    "        plt.plot(loss.epoch, loss.man_concat_acc, label='man_concat_acc')\n",
    "        plt.plot(loss.epoch, loss.val_man_concat_acc, label='val_man_concat_acc')\n",
    "        plt.legend()\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.savefig(os.path.join(save_dir, 'man_concat_acc.png'))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    if gradient:\n",
    "        plt.plot(loss.epoch, loss.gradient_norm, label='gradient_norm')\n",
    "        plt.legend()\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('gradient_norm')\n",
    "        plt.savefig(os.path.join(save_dir, 'gradient.png'))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "plot_keras_csv_logger(csv_logger, save_dir=model_save, accuracy=True, gradient = True)\n",
    "\n",
    "\n",
    "# # test saved model\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "path = model_save\n",
    "print(path)\n",
    "loss = pd.read_table(csv_logger.filename, delimiter=',')\n",
    "best_val_loss = np.min(loss.val_loss)\n",
    "best_epoch = np.argmin(loss.val_loss)\n",
    "\n",
    "\n",
    "# model_id =  'multi_190114123245'\n",
    "# path = os.path.join(FLAG.model_save,FLAG.target, model_id)\n",
    "# loss = pd.read_table(os.path.join(path, 'training.log'), delimiter=',')\n",
    "# best_val_loss = np.min(loss.val_loss)\n",
    "# best_epoch = np.argmin(loss.val_loss)\n",
    "# model_save = '.'\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "# evaluate validation set\n",
    "trained_model = load_model(os.path.join(path,'model.h5'))\n",
    "Pred = trained_model.predict(Xtest)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          save_dir=''):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    if not save_dir =='':\n",
    "        plt.savefig(os.path.join(save_dir))\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "# save into dictionary\n",
    "sav = vars(FLAG)\n",
    "sav['epoch'] = best_epoch\n",
    "sav['val_loss'] = best_val_loss\n",
    "sav['model_id'] = model_id\n",
    "\n",
    "c_list = ['management', 'UA', 'variability', 'deceleration', 'FHB','management_concat']\n",
    "for c in range(len(c_list)):\n",
    "    ypred_aug = np.argmax(Pred[c] , axis=1)\n",
    "    ytest_aug = np.argmax(Ytest[c], axis=1)\n",
    "\n",
    "    cfm = confusion_matrix(y_pred=ypred_aug, y_true=ytest_aug)\n",
    "    recall = np.diag(cfm) / np.sum(cfm, axis=1)\n",
    "    precision = np.diag(cfm) / np.sum(cfm, axis=0)\n",
    "    val_accu = accuracy_score(y_pred=ypred_aug, y_true=ytest_aug)\n",
    "    f1 = f1_score(y_pred=ypred_aug, y_true=ytest_aug, average=None)\n",
    "#         _val_recall = recall_score(val_targ, val_predict, average=None)\n",
    "#         _val_precision = precision_score(val_targ, val_predict, average=None)\n",
    "    \n",
    "    \n",
    "\n",
    "    print('%s accuracy : ' % (c_list[c]), val_accu)\n",
    "    tmp_list = []\n",
    "    for i in range(len(recall)):\n",
    "        print('%s_recall-%d : %.4f' %(c_list[c],i, recall[i]))\n",
    "        sav['%s_recall-%d' %(c_list[c], i)] = recall[i]\n",
    "        tmp_list.append(recall[i])\n",
    "    print('%s_recall-average : %.4f' %(c_list[c], sum(tmp_list)/len(tmp_list)))\n",
    "    sav['%s_recall-average' %(c_list[c])] = sum(tmp_list)/len(tmp_list)\n",
    "    \n",
    "    tmp_list = []\n",
    "    for i in range(len(precision)):\n",
    "        print('%s_precision-%d: %.4f' % (c_list[c],i, precision[i]))\n",
    "        sav['%s_precision-%d' %(c_list[c], i)] = precision[i]\n",
    "        tmp_list.append(precision[i])\n",
    "    print('%s_precision-average : %.4f' %(c_list[c], sum(tmp_list)/len(tmp_list)))\n",
    "    sav['%s_precision-average' %(c_list[c])] = sum(tmp_list)/len(tmp_list)\n",
    "    \n",
    "    tmp_list = []\n",
    "    for i in range(len(f1)):\n",
    "        print('%s_F1-%d: %.4f' % (c_list[c],i, f1[i]))\n",
    "        sav['%s_F1-%d' %(c_list[c], i)] = f1[i]\n",
    "        tmp_list.append(f1[i])\n",
    "    print('%s_F1-average : %.4f' %(c_list[c], sum(tmp_list)/len(tmp_list)))\n",
    "    sav['%s_F1-average' %(c_list[c])] = sum(tmp_list)/len(tmp_list)\n",
    "    \n",
    "    sav['val_%s_acc' %(c_list[c])] = val_accu\n",
    "\n",
    "    plt.figure()\n",
    "    rcl = [round(recall[i],2) for i in range(len(recall))]\n",
    "    plot_confusion_matrix(cfm, classes=np.arange(len(recall)),\n",
    "                          title='%s acc: %.2f \\nrecall: %s' %(c_list[c], val_accu,rcl),\n",
    "                          save_dir=model_save+'/%s_cm.png' % (c_list[c]) )\n",
    "# force management to 2 label \n",
    "c_list = ['management_2', 'management_concat_2']\n",
    "for idx, c in enumerate([0,5]):\n",
    "    # evaluate by every segment\n",
    "    ypred_aug = np.argmax(Pred[c] , axis=1)\n",
    "    ytest_aug = np.argmax(Ytest[c], axis=1)\n",
    "    ypred_aug[ypred_aug>1]=1\n",
    "    ytest_aug[ytest_aug>1]=1\n",
    "\n",
    "    cfm = confusion_matrix(y_pred=ypred_aug, y_true=ytest_aug)\n",
    "    recall = np.diag(cfm) / np.sum(cfm, axis=1)\n",
    "    precision = np.diag(cfm) / np.sum(cfm, axis=0)\n",
    "    val_accu = accuracy_score(y_pred=ypred_aug, y_true=ytest_aug)\n",
    "    f1 = f1_score(y_pred=ypred_aug, y_true=ytest_aug, average=None)\n",
    "    \n",
    "    tmp_list = []\n",
    "    for i in range(len(recall)):\n",
    "        print('%s_recall-%d : %.4f' %(c_list[idx],i, recall[i]))\n",
    "        sav['%s_recall-%d' %(c_list[idx], i)] = recall[i]\n",
    "        tmp_list.append(recall[i])\n",
    "    print('%s_recall-average : %.4f' %(c_list[idx], sum(tmp_list)/len(tmp_list)))\n",
    "    sav['%s_recall-average' %(c_list[idx])] = sum(tmp_list)/len(tmp_list)\n",
    "    \n",
    "    tmp_list = []\n",
    "    for i in range(len(precision)):\n",
    "        print('%s_precision-%d: %.4f' % (c_list[idx],i, precision[i]))\n",
    "        sav['%s_precision-%d' %(c_list[idx], i)] = precision[i]\n",
    "        tmp_list.append(precision[i])\n",
    "    print('%s_precision-average : %.4f' %(c_list[idx], sum(tmp_list)/len(tmp_list)))\n",
    "    sav['%s_precision-average' %(c_list[idx])] = sum(tmp_list)/len(tmp_list)\n",
    "    \n",
    "    tmp_list = []\n",
    "    for i in range(len(f1)):\n",
    "        print('%s_F1-%d: %.4f' % (c_list[idx],i, f1[i]))\n",
    "        sav['%s_F1-%d' %(c_list[idx], i)] = f1[i]\n",
    "        tmp_list.append(f1[i])\n",
    "    print('%s_F1-average : %.4f' %(c_list[idx], sum(tmp_list)/len(tmp_list)))\n",
    "    sav['%s_F1-average' %(c_list[idx])] = sum(tmp_list)/len(tmp_list)\n",
    "    \n",
    "    sav['val_%s_acc' %(c_list[idx])] = val_accu\n",
    "\n",
    "    plt.figure()\n",
    "    rcl = [round(recall[i],2) for i in range(len(recall))]\n",
    "    plot_confusion_matrix(cfm, classes=np.arange(len(recall)), \n",
    "                          title='%s acc: %.2f \\nrecall: %s' %(c_list[idx], val_accu,rcl),\n",
    "                          save_dir=model_save+'/%s_cm.png' % (c_list[idx]) )\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "# append into summary files\n",
    "dnew = pd.DataFrame(sav, index=[0])\n",
    "if os.path.exists(summary_save):\n",
    "    dori = pd.read_csv(summary_save)\n",
    "    dori = pd.concat([dori, dnew])\n",
    "    dori.to_csv(summary_save, index=False)\n",
    "else:\n",
    "    dnew.to_csv(summary_save, index=False)\n",
    "\n",
    "print(summary_save)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

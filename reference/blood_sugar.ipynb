{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import keras\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input,Conv1D, Dense, MaxPool1D, Activation, AvgPool1D,GlobalAveragePooling1D\n",
    "from keras.layers import Flatten, Add, Concatenate, Dropout, BatchNormalization\n",
    "from keras import regularizers, initializers\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats.stats import pearsonr\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from data_tools import *\n",
    "# from model_function import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multi_CNN = False\n",
    "activation = 'relu' # selu\n",
    "use_meta = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''read signal'''\n",
    "file_path = '/data/put_data/medical/BG/1006_data/'\n",
    "if multi_CNN:\n",
    "    with open(file_path+'dict_X_C10_S5_50.pickle', 'rb') as handle:\n",
    "        X50 = pickle.load(handle)\n",
    "    with open(file_path+'dict_X_C10_S5_100.pickle', 'rb') as handle: # 100\n",
    "        X5 = pickle.load(handle) \n",
    "    with open(file_path+'dict_X_C10_S5_200.pickle', 'rb') as handle: \n",
    "        X10 = pickle.load(handle) \n",
    "    with open(file_path+'dict_X_C10_S5_25.pickle', 'rb') as handle: \n",
    "        X25 = pickle.load(handle) \n",
    "else:\n",
    "    with open(file_path+'dict_X_C10_S5_200.pickle', 'rb') as handle:\n",
    "        X = pickle.load(handle)\n",
    "'''read meta'''\n",
    "with open(file_path+'dict_meta.pickle', 'rb') as handle:\n",
    "    meta = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(file_path+'dict_X_C10_all.pickle', 'rb') as handle:\n",
    "    X = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['011_1'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['011_1'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0 Person No Age Gender Height  SYS  DIA   HR    G  PWV   BMI  \\\n",
      "0          1         3  53      0    157  143  104   73  106  6.5  25.4   \n",
      "1          2         3  53      0    157  144   95   73   95  6.5  25.4   \n",
      "2          3         4  49      0    150  133   92   80   94    7  23.4   \n",
      "3          4         4  49      0    150  117   85   75   99    7  23.4   \n",
      "4          5         5  55      1  167.5  152   83  106   79    7  24.4   \n",
      "\n",
      "  BP_drug DM DM_drug O_drug W_cir weight     ID      Date  Time  \n",
      "0       0  0       0      1    83   62.7  003_1  20170302  0935  \n",
      "1       0  0       0      1    83   62.7  003_2  20170302  0939  \n",
      "2       0  0       0      1    80   52.6  004_1  20170303  0817  \n",
      "3       0  0       0      1    80   52.6  004_2  20170303  0827  \n",
      "4       0  0       0      0    91   68.5  005_1  20170303  0837  \n"
     ]
    }
   ],
   "source": [
    "# data split\n",
    "split_ratio = 0.8\n",
    "\n",
    "# model parameter\n",
    "l_2 = 0\n",
    "dropout_rate = 0.2\n",
    "lr_rate = 1e-5\n",
    "\n",
    "# Embedding\n",
    "n_sample = 6000\n",
    "n_meta = len(meta['003_1'])\n",
    "n_channel = 10\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 16\n",
    "dense_layers = [100,70,1]\n",
    "local_filters = 64\n",
    "local_kernel_size = 3\n",
    "local_pool_size = 2\n",
    "filters = 64\n",
    "strides = 1\n",
    "layers = 10\n",
    "\n",
    "# Training\n",
    "batch_size = 64\n",
    "n_batch_per_epoch = 200\n",
    "epochs = 200\n",
    "kernel_initializer = 'he_normal'\n",
    "\n",
    "d = pd.read_csv(\"/data/put_data/medical/BG/new_IRB_summary_1119_1006.csv\",dtype=\"str\")\n",
    "print(d[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Str'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'str'.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.columns = [col.capitalize() for col in d.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[['Id','Date','Time','Age','Gender','Height','Weight','Hr','G']].to_csv('/home/cmchang/BG/meta.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999_22\r"
     ]
    }
   ],
   "source": [
    "data_path = '/data/put_data/medical/BG/pro_data_AC'\n",
    "for sid in d.Id:\n",
    "    tmp = pd.read_csv(os.path.join(data_path, sid+'.csv'))\n",
    "    tmp.columns = ['MilliSecond', 'ECG', 'LR', 'LIR', 'RR','RIR']\n",
    "    tmp[['MilliSecond','ECG']].to_csv(os.path.join('/home/cmchang/BG/data/ECG',sid+'.csv'), index=False)\n",
    "    print(sid, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2238, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts of training:  1790  and counts of testing:  450\n"
     ]
    }
   ],
   "source": [
    "'''split training and testing data set'''\n",
    "D_id = list(set([row['Person No'] for index, row in d.iterrows() if float(row['G']) >= 200]))\n",
    "np.random.shuffle(D_id)\n",
    "ND_id = np.asarray(d.loc[np.asarray(d['G'], dtype = 'float64') < 200]['Person No'].unique())\n",
    "np.random.shuffle(ND_id)\n",
    "\n",
    "train_id = np.hstack((ND_id[:int(len(ND_id) * split_ratio)], D_id[:int(len(D_id) * split_ratio)]))\n",
    "# train_id = np.hstack((ND_id[:int(len(ND_id) * split_ratio)]))\n",
    "train_id\n",
    "\n",
    "test_id = np.hstack((ND_id[int(len(ND_id) * split_ratio):], D_id[int(len(D_id) * split_ratio):]))\n",
    "# test_id = np.hstack((ND_id[int(len(ND_id) * split_ratio):]))\n",
    "test_id\n",
    "\n",
    "test_ind = [index for index, row in d.iterrows() if np.any(test_id == row['Person No'])] \n",
    "np.random.shuffle(test_ind)\n",
    "train_ind = [index for index, row in d.iterrows() if np.any(train_id == row['Person No'])]   \n",
    "np.random.shuffle(train_ind)\n",
    "\n",
    "train_file = np.array(d['ID'][train_ind])\n",
    "test_file = np.array(d['ID'][test_ind])\n",
    "lab = np.array(d['G']).astype('float32')\n",
    "\n",
    "# train_lab = np.array(lab[train_ind])\n",
    "# test_lab  = np.array(lab[test_ind])\n",
    "\n",
    "train_lab = {f:d[d['ID'] == f]['G'].astype('float32').values for f in train_file}\n",
    "test_lab = {f:d[d['ID'] == f]['G'].astype('float32').values for f in test_file}\n",
    "print('counts of training: ', train_file.shape[0], ' and counts of testing: ', test_file.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model function define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ResidualBlock(filters,kernel_size,strides,pool_size,inputs):\n",
    "    if activation == 'selu':\n",
    "        new_input = MaxPool1D(pool_size=pool_size, padding = 'same', strides = strides)(inputs)\n",
    "        new_inp_2 = Activation(activation='selu')(inputs)\n",
    "        new_inp_2 = Dropout(dropout_rate)(new_inp_2)\n",
    "        new_inp_2 = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(new_inp_2)\n",
    "        new_inp_2 = Activation(activation='selu')(new_inp_2)\n",
    "        new_inp_2 = Dropout(dropout_rate)(new_inp_2)\n",
    "        new_inp_2 = Conv1D(filters = filters, kernel_size = kernel_size, strides = 1, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(new_inp_2)\n",
    "        new_inp_2 = Add()([new_inp_2, new_input])\n",
    "        return new_inp_2\n",
    "    else:\n",
    "        new_input = MaxPool1D(pool_size=pool_size, padding = 'same', strides = strides)(inputs)\n",
    "        new_inp_2 = BatchNormalization()(inputs)\n",
    "        new_inp_2 = Activation(activation=activation)(new_inp_2)\n",
    "        new_inp_2 = Dropout(dropout_rate)(new_inp_2)\n",
    "        new_inp_2 = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(new_inp_2)\n",
    "        new_inp_2 = BatchNormalization()(new_inp_2)\n",
    "        new_inp_2 = Activation(activation=activation)(new_inp_2)\n",
    "        new_inp_2 = Dropout(dropout_rate)(new_inp_2)\n",
    "        new_inp_2 = Conv1D(filters = filters, kernel_size = kernel_size, strides = 1, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(new_inp_2)\n",
    "        new_inp_2 = Add()([new_inp_2, new_input])\n",
    "        return new_inp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def local_cnn(filters,kernel_size,strides,pool_size,inputs):\n",
    "    if activation == 'selu':\n",
    "        inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer)(inputs)\n",
    "        inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer)(inp)\n",
    "        inp = MaxPool1D(pool_size = local_pool_size, padding = 'same')(inp)\n",
    "        inp = Activation(activation='selu')(inp)\n",
    "        inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer)(inp)\n",
    "        inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer)(inp)\n",
    "    else:\n",
    "        inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer)(inputs)\n",
    "        inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer)(inp)\n",
    "        inp = MaxPool1D(pool_size = local_pool_size, padding = 'same')(inp)\n",
    "        inp = BatchNormalization()(inp)\n",
    "        inp = Activation(activation=activation)(inp)\n",
    "        inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer)(inp)\n",
    "        inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer)(inp)\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    global strides, filters\n",
    "    print('Use multi CNN: ', multi_CNN, ' activation function: ', activation, '\\n')\n",
    "    print('Start building model.....')\n",
    "    if use_meta:\n",
    "        meta_inp = Input(shape=(n_meta,))\n",
    "    if multi_CNN:\n",
    "        sig_inp5 = Input(shape = (3000, 10))\n",
    "        inp5 = local_cnn(local_filters,local_kernel_size,strides,local_pool_size,sig_inp5)\n",
    "\n",
    "        sig_inp10 = Input(shape = (6000, 10))\n",
    "        inp10 = local_cnn(local_filters, local_kernel_size, strides, local_pool_size, sig_inp10)\n",
    "\n",
    "        sig_inp25 = Input(shape = (750, 10))\n",
    "        inp25 = local_cnn(local_filters, local_kernel_size, strides, local_pool_size, sig_inp25)\n",
    "\n",
    "        sig_inp50 = Input(shape = (1500, 10))\n",
    "        inp50 = local_cnn(local_filters, local_kernel_size, strides, local_pool_size, sig_inp50)\n",
    "        if activation == 'selu':\n",
    "            inp_con = keras.layers.concatenate([inp5, inp10, inp25, inp50], axis = 1)\n",
    "            inp_max = MaxPool1D(pool_size=2, padding = 'same')(inp_con)\n",
    "            l1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=2,padding=\"same\",kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(inp_con)\n",
    "            l1 = Activation(activation='selu')(l1)\n",
    "            l1 = Dropout(dropout_rate)(l1)\n",
    "            l1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=strides,padding=\"same\",kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(l1)\n",
    "            new_inp = Add()([l1,inp_max])\n",
    "\n",
    "            for i in range(layers):\n",
    "            # every alternate residual block subsample its input by a factor of 2\n",
    "                if i % 2 == 1:\n",
    "                    pool_size = 2\n",
    "                    strides = 2\n",
    "                else:\n",
    "                    pool_size = 1\n",
    "                    strides = 1\n",
    "                # incremented filters    \n",
    "                if i % 4 == 3:\n",
    "                    filters = 64*int(i//4 + 2)\n",
    "                    new_inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = 1, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "                new_inp = ResidualBlock(filters,kernel_size,strides,pool_size,new_inp)\n",
    "\n",
    "            new_inp = GlobalAveragePooling1D()(new_inp)\n",
    "            new_inp = Activation(activation='selu')(new_inp)\n",
    "            if use_meta:\n",
    "                new_inp = keras.layers.concatenate([new_inp,meta_inp])\n",
    "            new_inp = Dense(64,activation='selu', kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "            new_inp = Dropout(dropout_rate)(new_inp)\n",
    "            out = Dense(1,activation='linear', kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "        else:\n",
    "            inp_con = keras.layers.concatenate([inp5, inp10, inp25, inp50], axis = 1)\n",
    "            inp_max = MaxPool1D(pool_size=2, padding = 'same')(inp_con)\n",
    "            l1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=2,padding=\"same\",kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(inp_con)\n",
    "            l1 = BatchNormalization()(l1)\n",
    "            l1 = Activation(activation=activation)(l1)\n",
    "            l1 = Dropout(dropout_rate)(l1)\n",
    "            l1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=strides,padding=\"same\",kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(l1)\n",
    "            new_inp = Add()([l1,inp_max])\n",
    "\n",
    "            for i in range(layers):\n",
    "            # every alternate residual block subsample its input by a factor of 2\n",
    "                if i % 2 == 1:\n",
    "                    pool_size = 2\n",
    "                    strides = 2\n",
    "                else:\n",
    "                    pool_size = 1\n",
    "                    strides = 1\n",
    "                # incremented filters    \n",
    "                if i % 4 == 3:\n",
    "                    filters = 64*int(i//4 + 2)\n",
    "                    new_inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = 1, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "                new_inp = ResidualBlock(filters,kernel_size,strides,pool_size,new_inp)\n",
    "#             new_inp = Flatten()(new_inp)\n",
    "            new_inp = GlobalAveragePooling1D()(new_inp)\n",
    "            new_inp = BatchNormalization()(new_inp)\n",
    "            new_inp = Activation(activation=activation)(new_inp)\n",
    "#             new_inp = Dense(192,kernel_regularizer=regularizers.l2(l_2),kernel_initializer = kernel_initializer)(new_inp)\n",
    "            if use_meta:\n",
    "                new_inp = keras.layers.concatenate([new_inp,meta_inp])\n",
    "#             new_inp = BatchNormalization()(new_inp)\n",
    "#             new_inp = Activation(activation=activation)(new_inp)\n",
    "#             new_inp = Dropout(dropout_rate)(new_inp)\n",
    "#             new_inp = Dense(128,kernel_regularizer=regularizers.l2(l_2),kernel_initializer = kernel_initializer)(new_inp)\n",
    "#             new_inp = BatchNormalization()(new_inp)\n",
    "#             new_inp = Activation(activation=activation)(new_inp)\n",
    "#             new_inp = Dropout(dropout_rate)(new_inp)\n",
    "#             new_inp = Dense(128,kernel_regularizer=regularizers.l2(l_2),kernel_initializer = kernel_initializer)(new_inp)\n",
    "#             new_inp = BatchNormalization()(new_inp)\n",
    "#             new_inp = Activation(activation=activation)(new_inp)\n",
    "#             new_inp = Dropout(dropout_rate)(new_inp)\n",
    "            new_inp = Dense(128,activation=activation, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "            new_inp = Dropout(dropout_rate)(new_inp)\n",
    "            new_inp = Dense(128,activation=activation, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "            #new_inp = keras.layers.concatenate([new_inp,meta_inp])\n",
    "            out = Dense(1,activation='linear', kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "        if use_meta:\n",
    "            model = Model(inputs=[sig_inp5, sig_inp10, sig_inp25, sig_inp50,meta_inp],outputs=[out])\n",
    "        else:\n",
    "            model = Model(inputs=[sig_inp5, sig_inp10, sig_inp25, sig_inp50],outputs=[out])\n",
    "    else:\n",
    "        sig_inp =  Input(shape=(n_sample, n_channel))        \n",
    "        if activation == 'selu':\n",
    "            inp = Conv1D(filters=filters,kernel_size=kernel_size,strides=strides,padding=\"same\")(sig_inp)\n",
    "            inp = Activation(activation='selu')(inp)\n",
    "            inp_max = MaxPool1D(pool_size=2)(inp)\n",
    "\n",
    "            l1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=2,padding=\"same\")(inp)\n",
    "            l1 = Activation(activation='selu')(l1)\n",
    "            l1 = Dropout(dropout_rate)(l1)\n",
    "            l1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=strides,padding=\"same\")(l1)\n",
    "            new_inp = Add()([l1,inp_max])\n",
    "\n",
    "            for i in range(layers):\n",
    "            # every alternate residual block subsample its input by a factor of 2\n",
    "                if i % 2 == 1:\n",
    "                    pool_size = 2\n",
    "                    strides = 2\n",
    "                else:\n",
    "                    pool_size = 1\n",
    "                    strides = 1\n",
    "                # incremented filters    \n",
    "                if i % 4 == 3:\n",
    "                    filters = 64*int(i//4 + 2)\n",
    "                    new_inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = 1, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "                new_inp = ResidualBlock(filters,kernel_size,strides,pool_size,new_inp)\n",
    "\n",
    "            new_inp = GlobalAveragePooling1D()(new_inp)\n",
    "            new_inp = Activation(activation='selu')(new_inp)\n",
    "            if use_meta:\n",
    "                new_inp = keras.layers.concatenate([new_inp,meta_inp])\n",
    "            new_inp = Dense(256,activation='selu', kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "            new_inp = Dropout(dropout_rate)(new_inp)\n",
    "            out = Dense(1,activation='linear', kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "        else:\n",
    "            inp = Conv1D(filters=filters,kernel_size=kernel_size,strides=strides,padding=\"same\")(sig_inp)\n",
    "            inp = BatchNormalization()(inp)\n",
    "            inp = Activation(activation=activation)(inp)\n",
    "            inp_max = MaxPool1D(pool_size=2)(inp)\n",
    "\n",
    "            l1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=2,padding=\"same\")(inp)\n",
    "            l1 = BatchNormalization()(l1)\n",
    "            l1 = Activation(activation=activation)(l1)\n",
    "            l1 = Dropout(dropout_rate)(l1)\n",
    "            l1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=strides,padding=\"same\")(l1)\n",
    "\n",
    "            new_inp = Add()([l1,inp_max])\n",
    "\n",
    "            for i in range(layers):\n",
    "            # every alternate residual block subsample its input by a factor of 2\n",
    "                if i % 2 == 1:\n",
    "                    pool_size = 2\n",
    "                    strides = 2\n",
    "                else:\n",
    "                    pool_size = 1\n",
    "                    strides = 1\n",
    "                # incremented filters    \n",
    "                if i % 4 == 3:\n",
    "                    filters = 64*int(i//4 + 2)\n",
    "                    new_inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = 1, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "                new_inp = ResidualBlock(filters,kernel_size,strides,pool_size,new_inp)\n",
    "\n",
    "            new_inp = GlobalAveragePooling1D()(new_inp)\n",
    "            new_inp = BatchNormalization()(new_inp)\n",
    "            new_inp = Activation(activation=activation)(new_inp)\n",
    "            if use_meta:\n",
    "                new_inp = keras.layers.concatenate([new_inp,meta_inp])\n",
    "            new_inp = Dense(128,activation=activation, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "            new_inp = Dropout(dropout_rate)(new_inp)\n",
    "            new_inp = Dense(128,activation=activation, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "            #new_inp = keras.layers.concatenate([new_inp,meta_inp])\n",
    "            out = Dense(1,activation='linear', kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "        if use_meta:\n",
    "            model = Model(inputs=[sig_inp,meta_inp],outputs=[out])\n",
    "        else:\n",
    "            model = Model(inputs=[sig_inp],outputs=[out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use multi CNN:  False  activation function:  relu \n",
      "\n",
      "Start building model.....\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_6 (InputLayer)             (None, 6000, 10)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)               (None, 6000, 64)      10304       input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNor (None, 6000, 64)      256         conv1d_51[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_47 (Activation)       (None, 6000, 64)      0           batch_normalization_24[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)               (None, 3000, 64)      65600       activation_47[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNor (None, 3000, 64)      256         conv1d_52[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_48 (Activation)       (None, 3000, 64)      0           batch_normalization_25[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)             (None, 3000, 64)      0           activation_48[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)               (None, 3000, 64)      65600       dropout_45[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D)  (None, 3000, 64)      0           activation_47[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_23 (Add)                     (None, 3000, 64)      0           conv1d_53[0][0]                  \n",
      "                                                                   max_pooling1d_23[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNor (None, 3000, 64)      256         add_23[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_49 (Activation)       (None, 3000, 64)      0           batch_normalization_26[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)             (None, 3000, 64)      0           activation_49[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)               (None, 3000, 64)      65600       dropout_46[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNor (None, 3000, 64)      256         conv1d_54[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_50 (Activation)       (None, 3000, 64)      0           batch_normalization_27[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)             (None, 3000, 64)      0           activation_50[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)               (None, 3000, 64)      65600       dropout_47[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D)  (None, 3000, 64)      0           add_23[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "add_24 (Add)                     (None, 3000, 64)      0           conv1d_55[0][0]                  \n",
      "                                                                   max_pooling1d_24[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNor (None, 3000, 64)      256         add_24[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_51 (Activation)       (None, 3000, 64)      0           batch_normalization_28[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)             (None, 3000, 64)      0           activation_51[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)               (None, 1500, 64)      65600       dropout_48[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNor (None, 1500, 64)      256         conv1d_56[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_52 (Activation)       (None, 1500, 64)      0           batch_normalization_29[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)             (None, 1500, 64)      0           activation_52[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)               (None, 1500, 64)      65600       dropout_49[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D)  (None, 1500, 64)      0           add_24[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "add_25 (Add)                     (None, 1500, 64)      0           conv1d_57[0][0]                  \n",
      "                                                                   max_pooling1d_25[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNor (None, 1500, 64)      256         add_25[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_53 (Activation)       (None, 1500, 64)      0           batch_normalization_30[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)             (None, 1500, 64)      0           activation_53[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)               (None, 1500, 64)      65600       dropout_50[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNor (None, 1500, 64)      256         conv1d_58[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_54 (Activation)       (None, 1500, 64)      0           batch_normalization_31[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)             (None, 1500, 64)      0           activation_54[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)               (None, 1500, 64)      65600       dropout_51[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D)  (None, 1500, 64)      0           add_25[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "add_26 (Add)                     (None, 1500, 64)      0           conv1d_59[0][0]                  \n",
      "                                                                   max_pooling1d_26[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)               (None, 1500, 128)     131200      add_26[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNor (None, 1500, 128)     512         conv1d_60[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_55 (Activation)       (None, 1500, 128)     0           batch_normalization_32[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)             (None, 1500, 128)     0           activation_55[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)               (None, 750, 128)      262272      dropout_52[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNor (None, 750, 128)      512         conv1d_61[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_56 (Activation)       (None, 750, 128)      0           batch_normalization_33[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)             (None, 750, 128)      0           activation_56[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)               (None, 750, 128)      262272      dropout_53[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D)  (None, 750, 128)      0           conv1d_60[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_27 (Add)                     (None, 750, 128)      0           conv1d_62[0][0]                  \n",
      "                                                                   max_pooling1d_27[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNor (None, 750, 128)      512         add_27[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_57 (Activation)       (None, 750, 128)      0           batch_normalization_34[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)             (None, 750, 128)      0           activation_57[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)               (None, 750, 128)      262272      dropout_54[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNor (None, 750, 128)      512         conv1d_63[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_58 (Activation)       (None, 750, 128)      0           batch_normalization_35[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)             (None, 750, 128)      0           activation_58[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)               (None, 750, 128)      262272      dropout_55[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D)  (None, 750, 128)      0           add_27[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "add_28 (Add)                     (None, 750, 128)      0           conv1d_64[0][0]                  \n",
      "                                                                   max_pooling1d_28[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNor (None, 750, 128)      512         add_28[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_59 (Activation)       (None, 750, 128)      0           batch_normalization_36[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)             (None, 750, 128)      0           activation_59[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)               (None, 375, 128)      262272      dropout_56[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNor (None, 375, 128)      512         conv1d_65[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_60 (Activation)       (None, 375, 128)      0           batch_normalization_37[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)             (None, 375, 128)      0           activation_60[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)               (None, 375, 128)      262272      dropout_57[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D)  (None, 375, 128)      0           add_28[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "add_29 (Add)                     (None, 375, 128)      0           conv1d_66[0][0]                  \n",
      "                                                                   max_pooling1d_29[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNor (None, 375, 128)      512         add_29[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_61 (Activation)       (None, 375, 128)      0           batch_normalization_38[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)             (None, 375, 128)      0           activation_61[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)               (None, 375, 128)      262272      dropout_58[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNor (None, 375, 128)      512         conv1d_67[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_62 (Activation)       (None, 375, 128)      0           batch_normalization_39[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)             (None, 375, 128)      0           activation_62[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)               (None, 375, 128)      262272      dropout_59[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D)  (None, 375, 128)      0           add_29[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "add_30 (Add)                     (None, 375, 128)      0           conv1d_68[0][0]                  \n",
      "                                                                   max_pooling1d_30[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_69 (Conv1D)               (None, 375, 192)      393408      add_30[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNor (None, 375, 192)      768         conv1d_69[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_63 (Activation)       (None, 375, 192)      0           batch_normalization_40[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)             (None, 375, 192)      0           activation_63[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_70 (Conv1D)               (None, 188, 192)      590016      dropout_60[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNor (None, 188, 192)      768         conv1d_70[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_64 (Activation)       (None, 188, 192)      0           batch_normalization_41[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)             (None, 188, 192)      0           activation_64[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_71 (Conv1D)               (None, 188, 192)      590016      dropout_61[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D)  (None, 188, 192)      0           conv1d_69[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_31 (Add)                     (None, 188, 192)      0           conv1d_71[0][0]                  \n",
      "                                                                   max_pooling1d_31[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNor (None, 188, 192)      768         add_31[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_65 (Activation)       (None, 188, 192)      0           batch_normalization_42[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)             (None, 188, 192)      0           activation_65[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_72 (Conv1D)               (None, 188, 192)      590016      dropout_62[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNor (None, 188, 192)      768         conv1d_72[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_66 (Activation)       (None, 188, 192)      0           batch_normalization_43[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)             (None, 188, 192)      0           activation_66[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_73 (Conv1D)               (None, 188, 192)      590016      dropout_63[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D)  (None, 188, 192)      0           add_31[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "add_32 (Add)                     (None, 188, 192)      0           conv1d_73[0][0]                  \n",
      "                                                                   max_pooling1d_32[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNor (None, 188, 192)      768         add_32[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_67 (Activation)       (None, 188, 192)      0           batch_normalization_44[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)             (None, 188, 192)      0           activation_67[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_74 (Conv1D)               (None, 94, 192)       590016      dropout_64[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNor (None, 94, 192)       768         conv1d_74[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_68 (Activation)       (None, 94, 192)       0           batch_normalization_45[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)             (None, 94, 192)       0           activation_68[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_75 (Conv1D)               (None, 94, 192)       590016      dropout_65[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D)  (None, 94, 192)       0           add_32[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "add_33 (Add)                     (None, 94, 192)       0           conv1d_75[0][0]                  \n",
      "                                                                   max_pooling1d_33[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glob (None, 192)           0           add_33[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNor (None, 192)           768         global_average_pooling1d_3[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "activation_69 (Activation)       (None, 192)           0           batch_normalization_46[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "input_5 (InputLayer)             (None, 14)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 206)           0           activation_69[0][0]              \n",
      "                                                                   input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 128)           26496       concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)             (None, 128)           0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 128)           16512       dropout_66[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             129         dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 6,752,641\n",
      "Trainable params: 6,746,881\n",
      "Non-trainable params: 5,760\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "adam = Adam(lr=lr_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay = 0.0)\n",
    "model.compile(loss='mae', optimizer=adam)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory and Model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be saved to  train_1012/D2_l0_multicnn=False_L23_GVP_Meta=True_kernal=16/ ...........\n"
     ]
    }
   ],
   "source": [
    "'''model name'''\n",
    "time_str = time.strftime(\"%m%d\")\n",
    "drop = str(math.ceil(dropout_rate/0.1))\n",
    "model_name =  'D' + drop + '_l' + str(l_2) +'_multicnn='+str(multi_CNN)+'_L'+str(layers*2+3)+'_GVP_Meta=' + str(use_meta) + '_kernal=' + str(kernel_size)\n",
    "directory = 'train_' + time_str + '/' + model_name + '/' \n",
    "\n",
    "'''model training'''\n",
    "print('Model will be saved to ', directory, '...........')\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(directory + \"model.h5\",\n",
    "                    monitor='val_loss',\n",
    "                    verbose=1,\n",
    "                    save_best_only=True,\n",
    "                    mode='auto')\n",
    "\n",
    "def figure_making(train_loss, val_loss):\n",
    "    sk = 0\n",
    "    plt.figure(0)\n",
    "    plt.plot(range(len(train_loss[sk:])),train_loss[sk:],label='train_loss')\n",
    "    plt.plot(range(len(val_loss[sk:])),val_loss[sk:],label='val_loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.ylim([0, max(train_loss+val_loss)])\n",
    "    plt.savefig(directory + \"loss_history.png\", dpi = 300)\n",
    "    plt.close()\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self,logs={}):\n",
    "        self.loss=[]\n",
    "        self.val_loss=[]\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        if len(self.loss) % 30 == 0 and len(self.loss) != 0:\n",
    "            figure_making(self.loss, self.val_loss)\n",
    "\n",
    "loss_history = LossHistory()\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience = 30, mode = 'auto')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor = 0.3, patience = 5, min_lr = 0, cooldown = 5, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# records =  2250\n",
      "Epoch 1/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 106.8145Epoch 00000: val_loss improved from inf to 103.65671, saving model to train_1012/D2_l0_multicnn=False_L23_GVP_Meta=True_kernal=16/model.h5\n",
      "200/200 [==============================] - 118s - loss: 106.8069 - val_loss: 103.6567\n",
      "Epoch 2/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 104.5284Epoch 00001: val_loss improved from 103.65671 to 100.50030, saving model to train_1012/D2_l0_multicnn=False_L23_GVP_Meta=True_kernal=16/model.h5\n",
      "200/200 [==============================] - 77s - loss: 104.5313 - val_loss: 100.5003\n",
      "Epoch 3/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 102.0653Epoch 00002: val_loss improved from 100.50030 to 97.61952, saving model to train_1012/D2_l0_multicnn=False_L23_GVP_Meta=True_kernal=16/model.h5\n",
      "200/200 [==============================] - 77s - loss: 102.0654 - val_loss: 97.6195\n",
      "Epoch 4/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 98.8558Epoch 00003: val_loss improved from 97.61952 to 94.78201, saving model to train_1012/D2_l0_multicnn=False_L23_GVP_Meta=True_kernal=16/model.h5\n",
      "200/200 [==============================] - 77s - loss: 98.8383 - val_loss: 94.7820\n",
      "Epoch 5/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 94.9583Epoch 00004: val_loss improved from 94.78201 to 90.63598, saving model to train_1012/D2_l0_multicnn=False_L23_GVP_Meta=True_kernal=16/model.h5\n",
      "200/200 [==============================] - 77s - loss: 94.9525 - val_loss: 90.6360\n",
      "Epoch 6/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 89.9063Epoch 00005: val_loss improved from 90.63598 to 85.17750, saving model to train_1012/D2_l0_multicnn=False_L23_GVP_Meta=True_kernal=16/model.h5\n",
      "200/200 [==============================] - 77s - loss: 89.8890 - val_loss: 85.1775\n",
      "Epoch 7/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 83.2032Epoch 00006: val_loss improved from 85.17750 to 77.56372, saving model to train_1012/D2_l0_multicnn=False_L23_GVP_Meta=True_kernal=16/model.h5\n",
      "200/200 [==============================] - 77s - loss: 83.1568 - val_loss: 77.5637\n",
      "Epoch 8/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 74.3404Epoch 00007: val_loss improved from 77.56372 to 68.86427, saving model to train_1012/D2_l0_multicnn=False_L23_GVP_Meta=True_kernal=16/model.h5\n",
      "200/200 [==============================] - 77s - loss: 74.3349 - val_loss: 68.8643\n",
      "Epoch 9/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 62.8929Epoch 00008: val_loss improved from 68.86427 to 56.91674, saving model to train_1012/D2_l0_multicnn=False_L23_GVP_Meta=True_kernal=16/model.h5\n",
      "200/200 [==============================] - 77s - loss: 62.8709 - val_loss: 56.9167\n",
      "Epoch 10/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 49.5892Epoch 00009: val_loss improved from 56.91674 to 42.91664, saving model to train_1012/D2_l0_multicnn=False_L23_GVP_Meta=True_kernal=16/model.h5\n",
      "200/200 [==============================] - 77s - loss: 49.6039 - val_loss: 42.9166\n",
      "Epoch 11/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 34.6051Epoch 00010: val_loss improved from 42.91664 to 30.59374, saving model to train_1012/D2_l0_multicnn=False_L23_GVP_Meta=True_kernal=16/model.h5\n",
      "200/200 [==============================] - 77s - loss: 34.5720 - val_loss: 30.5937\n",
      "Epoch 12/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 21.6146Epoch 00011: val_loss improved from 30.59374 to 23.97100, saving model to train_1012/D2_l0_multicnn=False_L23_GVP_Meta=True_kernal=16/model.h5\n",
      "200/200 [==============================] - 77s - loss: 21.5738 - val_loss: 23.9710\n",
      "Epoch 13/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 16.3246Epoch 00012: val_loss improved from 23.97100 to 20.00869, saving model to train_1012/D2_l0_multicnn=False_L23_GVP_Meta=True_kernal=16/model.h5\n",
      "200/200 [==============================] - 77s - loss: 16.3260 - val_loss: 20.0087\n",
      "Epoch 14/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 14.9679Epoch 00013: val_loss did not improve\n",
      "200/200 [==============================] - 76s - loss: 14.9509 - val_loss: 21.2925\n",
      "Epoch 15/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 14.6696Epoch 00014: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 14.6781 - val_loss: 20.5469\n",
      "Epoch 16/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 14.3309Epoch 00015: val_loss improved from 20.00869 to 17.93518, saving model to train_1012/D2_l0_multicnn=False_L23_GVP_Meta=True_kernal=16/model.h5\n",
      "200/200 [==============================] - 77s - loss: 14.3344 - val_loss: 17.9352\n",
      "Epoch 17/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 14.2140Epoch 00016: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 14.2087 - val_loss: 19.8604\n",
      "Epoch 18/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 14.2231Epoch 00017: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 14.2312 - val_loss: 18.6237\n",
      "Epoch 19/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 14.1155Epoch 00018: val_loss improved from 17.93518 to 17.90377, saving model to train_1012/D2_l0_multicnn=False_L23_GVP_Meta=True_kernal=16/model.h5\n",
      "200/200 [==============================] - 77s - loss: 14.1309 - val_loss: 17.9038\n",
      "Epoch 20/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 13.9121Epoch 00019: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 13.9170 - val_loss: 18.6452\n",
      "Epoch 21/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 13.8597Epoch 00020: val_loss did not improve\n",
      "200/200 [==============================] - 76s - loss: 13.8610 - val_loss: 19.3775\n",
      "Epoch 22/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 13.7662Epoch 00021: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 13.7664 - val_loss: 18.9881\n",
      "Epoch 23/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 13.6400Epoch 00022: val_loss did not improve\n",
      "200/200 [==============================] - 76s - loss: 13.6374 - val_loss: 18.8881\n",
      "Epoch 24/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 13.4988Epoch 00023: val_loss did not improve\n",
      "200/200 [==============================] - 76s - loss: 13.5056 - val_loss: 21.7422\n",
      "Epoch 25/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 13.3509Epoch 00024: val_loss did not improve\n",
      "\n",
      "Epoch 00024: reducing learning rate to 2.9999999242136253e-06.\n",
      "200/200 [==============================] - 78s - loss: 13.3454 - val_loss: 20.4830\n",
      "Epoch 26/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 13.2031Epoch 00025: val_loss did not improve\n",
      "200/200 [==============================] - 76s - loss: 13.2320 - val_loss: 21.6961\n",
      "Epoch 27/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 13.0467Epoch 00026: val_loss did not improve\n",
      "200/200 [==============================] - 76s - loss: 13.0364 - val_loss: 21.3960\n",
      "Epoch 28/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 13.0660Epoch 00027: val_loss did not improve\n",
      "200/200 [==============================] - 76s - loss: 13.0644 - val_loss: 22.8494\n",
      "Epoch 29/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.9107Epoch 00028: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 12.9064 - val_loss: 20.6389\n",
      "Epoch 30/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.7635Epoch 00029: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 12.7576 - val_loss: 22.3216\n",
      "Epoch 31/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.7882Epoch 00030: val_loss did not improve\n",
      "200/200 [==============================] - 76s - loss: 12.7758 - val_loss: 21.2281\n",
      "Epoch 32/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.7458Epoch 00031: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 12.7478 - val_loss: 22.5688\n",
      "Epoch 33/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.7048Epoch 00032: val_loss did not improve\n",
      "200/200 [==============================] - 78s - loss: 12.6908 - val_loss: 21.5836\n",
      "Epoch 34/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.6836Epoch 00033: val_loss did not improve\n",
      "200/200 [==============================] - 78s - loss: 12.6969 - val_loss: 21.3691\n",
      "Epoch 35/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.4967Epoch 00034: val_loss did not improve\n",
      "\n",
      "Epoch 00034: reducing learning rate to 8.999999636216671e-07.\n",
      "200/200 [==============================] - 77s - loss: 12.4902 - val_loss: 21.7351\n",
      "Epoch 36/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.4774Epoch 00035: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 12.4749 - val_loss: 22.8027\n",
      "Epoch 37/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.2822Epoch 00036: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 12.2748 - val_loss: 22.1311\n",
      "Epoch 38/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.4014Epoch 00037: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 12.4074 - val_loss: 22.7975\n",
      "Epoch 39/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.5315Epoch 00038: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 12.5198 - val_loss: 22.2657\n",
      "Epoch 40/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.3969Epoch 00039: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 12.4015 - val_loss: 22.4571\n",
      "Epoch 41/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.3338Epoch 00040: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 12.3455 - val_loss: 22.3951\n",
      "Epoch 42/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.3387Epoch 00041: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 12.3455 - val_loss: 22.4225\n",
      "Epoch 43/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.4798Epoch 00042: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 12.5071 - val_loss: 21.8024\n",
      "Epoch 44/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.2984Epoch 00043: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 12.2997 - val_loss: 22.1755\n",
      "Epoch 45/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.3483Epoch 00044: val_loss did not improve\n",
      "\n",
      "Epoch 00044: reducing learning rate to 2.6999999249710525e-07.\n",
      "200/200 [==============================] - 78s - loss: 12.3312 - val_loss: 22.2042\n",
      "Epoch 46/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.2659Epoch 00045: val_loss did not improve\n",
      "200/200 [==============================] - 78s - loss: 12.2838 - val_loss: 22.4214\n",
      "Epoch 47/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.3043Epoch 00046: val_loss did not improve\n",
      "200/200 [==============================] - 77s - loss: 12.2945 - val_loss: 22.3363\n",
      "Epoch 48/200\n",
      "113/200 [===============>..............] - ETA: 32s - loss: 12.2568"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-38c3f2f18892>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_batch_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                         callbacks = [loss_history,checkpoint,earlystopping,reduce_lr])\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mVal_X50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVal_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generation_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_lab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1838\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1839\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1563\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if multi_CNN:\n",
    "    if use_meta:\n",
    "        Val_X5, Val_X10, Val\n",
    "        _X25, Val_X50, Val_M,Val_Y = validation_generation(test_file,test_lab,multi_CNN,use_meta,X5,X10,X25,X50,meta)\n",
    "        model.fit_generator(generator=train_generator(train_file,batch_size,multi_CNN,use_meta,X5,X10,X25,X50,meta,train_lab),\n",
    "                        validation_data=([Val_X5, Val_X10, Val_X25,Val_X50,Val_M],Val_Y),\n",
    "                        steps_per_epoch = n_batch_per_epoch,\n",
    "                        epochs=epochs,\n",
    "                        callbacks = [loss_history,checkpoint,earlystopping,reduce_lr])\n",
    "    else:\n",
    "        Val_X5, Val_X10, Val_X25, Val_X50,Val_Y = validation_generation(test_file,test_lab,multi_CNN,use_meta,X5,X10,X25,X50,meta)\n",
    "        model.fit_generator(generator=train_generator(train_file,batch_size,multi_CNN,use_meta,X5,X10,X25,X50,meta,train_lab),\n",
    "                                validation_data=([Val_X5, Val_X10, Val_X25,Val_X50],Val_Y),\n",
    "                                steps_per_epoch = n_batch_per_epoch,\n",
    "                                epochs=epochs,\n",
    "                                callbacks = [loss_history,checkpoint,earlystopping,reduce_lr])\n",
    "else:\n",
    "    if use_meta:\n",
    "        Val_X, Val_M,Val_Y = validation_generation_single(test_file,test_lab,X,meta,use_meta)\n",
    "        model.fit_generator(generator=train_generator_single(train_file, batch_size, X, meta, train_lab, use_meta),\n",
    "                        validation_data=([Val_X,Val_M],Val_Y),\n",
    "                        steps_per_epoch = n_batch_per_epoch,\n",
    "                        epochs=epochs,\n",
    "                        callbacks = [loss_history,checkpoint,earlystopping,reduce_lr])\n",
    "    else:\n",
    "        Val_X50,Val_Y = validation_generation_single(test_file,test_lab,X,meta,use_meta)\n",
    "        model.fit_generator(generator=train_generator_single(train_file, batch_size, X, meta, train_lab, use_meta),\n",
    "                            validation_data=([Val_X50],Val_Y),\n",
    "                            steps_per_epoch = n_batch_per_epoch,\n",
    "                            epochs=epochs,\n",
    "                            callbacks = [loss_history,checkpoint,earlystopping,reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting and Saving DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/matplotlib/__init__.py:1405: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "'''loss history fig'''\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "sk = 0\n",
    "plt.figure(0)\n",
    "plt.plot(range(len(loss_history.loss[sk:])),loss_history.loss[sk:],label='train_loss')\n",
    "plt.plot(range(len(loss_history.val_loss[sk:])),loss_history.val_loss[sk:],label='val_loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim([0, max(loss_history.val_loss+loss_history.loss)])\n",
    "plt.savefig(directory + \"loss_history.png\", dpi = 300)\n",
    "plt.close()\n",
    "\n",
    "'''plot train/test mae/cor fig'''\n",
    "tmp = [model_name]\n",
    "\n",
    "# recording\n",
    "tmp.append(loss_history.loss[loss_history.val_loss.index(min(loss_history.val_loss))])\n",
    "tmp.append(min(loss_history.val_loss))\n",
    "tmp.append(loss_history.loss[-1])\n",
    "tmp.append(loss_history.val_loss[-1])\n",
    "\n",
    "model_test = load_model(directory + \"model.h5\")\n",
    "\n",
    "# plot training\n",
    "if use_meta and multi_CNN:\n",
    "    train_X1, train_X2, train_X3, train_X4, train_M,train_Y1 = one_train_set(multi_CNN,use_meta,X5,X10,X25,X50,meta,train_lab,train_file=train_file)\n",
    "    Y_pred = model_test.predict([train_X1, train_X2, train_X3, train_X4,train_M])\n",
    "else:\n",
    "    train_X1, train_meta, train_Y1 = one_train_set_single(X,meta,train_lab,train_file,use_meta,size = 2000)\n",
    "    Y_pred = model_test.predict([train_X1,train_meta])\n",
    "Y_pred = np.hstack(Y_pred)\n",
    "\n",
    "cor = pearsonr(train_Y1,Y_pred)[0]\n",
    "mae = mean_absolute_error(train_Y1,Y_pred)\n",
    "\n",
    "# recording\n",
    "tmp.append(mae)\n",
    "tmp.append(cor)\n",
    "\n",
    "\n",
    "f = pd.DataFrame({'y_true': train_Y1,'y_pred': Y_pred})\n",
    "f.to_csv(directory + \"pred_train.csv\")\n",
    "\n",
    "\n",
    "plt.figure(0)\n",
    "plt.scatter(train_Y1, Y_pred, alpha = .15, s = 20)\n",
    "plt.xlabel('True_Y')\n",
    "plt.ylabel('Pred_Y')\n",
    "plt.title(\"Training data \\n\" + \"MAE = %4f; Cor = %4f; #samples = %d\" % (mae,cor, len(train_Y1)))\n",
    "plt.savefig(directory+\"plot_scatter_train.png\", dpi = 300)\n",
    "plt.close()\n",
    "\n",
    "# plot testing\n",
    "if use_meta and multi_CNN:\n",
    "    Y_pred = model_test.predict([Val_X5, Val_X10, Val_X25, Val_X50, Val_M])\n",
    "else:\n",
    "    Y_pred = model_test.predict([Val_X, Val_M])\n",
    "    \n",
    "Y_pred = np.hstack(Y_pred)\n",
    "\n",
    "f = pd.DataFrame({'y_true': Val_Y,'y_pred': Y_pred})\n",
    "f.to_csv(directory + \"pred_test.csv\")\n",
    "\n",
    "cor = pearsonr(Val_Y,Y_pred)[0]\n",
    "mae = mean_absolute_error(Val_Y,Y_pred)\n",
    "\n",
    "# recording\n",
    "tmp.append(mae)\n",
    "tmp.append(cor)\n",
    "\n",
    "'''scatter plot for test data'''\n",
    "plt.figure(0)\n",
    "plt.scatter(Val_Y, Y_pred, alpha = .15, s = 20)\n",
    "plt.xlabel('True_Y')\n",
    "plt.ylabel('Pred_Y')\n",
    "plt.title(\"Testing data \\n\" + \"MAE = %4f; Cor = %4f; #samples = %d\" % (mae, cor, len(Val_Y)))\n",
    "plt.savefig(directory + \"plot_scatter_test.png\", dpi = 300)\n",
    "plt.close()\n",
    "\n",
    "'''save model detail'''\n",
    "f = pd.DataFrame([tmp], columns = ['model_name', 'train_loss', 'val_loss', 'final_train_loss', 'final_val_loss', 'train_mae', 'train_cor', 'test_mae', 'test_cor'])\n",
    "f.to_csv(directory + 'model_detail.csv')\n",
    "\n",
    "'''save test ID'''\n",
    "f2 = pd.DataFrame([ID for ID in test_file], columns = ['test_ID'])\n",
    "f2.to_csv(directory + 'test_id.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Output Flatten Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 6000, 10)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 6000, 64)      20544       input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 6000, 64)      256         conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 6000, 64)      0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 3000, 64)      131136      activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 3000, 64)      256         conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 3000, 64)      0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 3000, 64)      0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 3000, 64)      131136      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 3000, 64)      0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 3000, 64)      0           conv1d_3[0][0]                   \n",
      "                                                                   max_pooling1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 3000, 64)      256         add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 3000, 64)      0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 3000, 64)      0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)                (None, 3000, 64)      131136      dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 3000, 64)      256         conv1d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 3000, 64)      0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 3000, 64)      0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)                (None, 3000, 64)      131136      dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 3000, 64)      0           add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 3000, 64)      0           conv1d_5[0][0]                   \n",
      "                                                                   max_pooling1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 3000, 64)      256         add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 3000, 64)      0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 3000, 64)      0           activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)                (None, 1500, 64)      131136      dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 1500, 64)      256         conv1d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 1500, 64)      0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 1500, 64)      0           activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)                (None, 1500, 64)      131136      dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)   (None, 1500, 64)      0           add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, 1500, 64)      0           conv1d_7[0][0]                   \n",
      "                                                                   max_pooling1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 1500, 64)      256         add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 1500, 64)      0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 1500, 64)      0           activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)                (None, 1500, 64)      131136      dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 1500, 64)      256         conv1d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 1500, 64)      0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 1500, 64)      0           activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)                (None, 1500, 64)      131136      dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)   (None, 1500, 64)      0           add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_4 (Add)                      (None, 1500, 64)      0           conv1d_9[0][0]                   \n",
      "                                                                   max_pooling1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)               (None, 1500, 128)     262272      add_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 1500, 128)     512         conv1d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 1500, 128)     0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 1500, 128)     0           activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)               (None, 750, 128)      524416      dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 750, 128)      512         conv1d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 750, 128)      0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 750, 128)      0           activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)               (None, 750, 128)      524416      dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)   (None, 750, 128)      0           conv1d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_5 (Add)                      (None, 750, 128)      0           conv1d_12[0][0]                  \n",
      "                                                                   max_pooling1d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 750, 128)      512         add_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 750, 128)      0           batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 750, 128)      0           activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)               (None, 750, 128)      524416      dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 750, 128)      512         conv1d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 750, 128)      0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 750, 128)      0           activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)               (None, 750, 128)      524416      dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)   (None, 750, 128)      0           add_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_6 (Add)                      (None, 750, 128)      0           conv1d_14[0][0]                  \n",
      "                                                                   max_pooling1d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 750, 128)      512         add_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 750, 128)      0           batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 750, 128)      0           activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)               (None, 375, 128)      524416      dropout_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, 375, 128)      512         conv1d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 375, 128)      0           batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 375, 128)      0           activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)               (None, 375, 128)      524416      dropout_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)   (None, 375, 128)      0           add_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_7 (Add)                      (None, 375, 128)      0           conv1d_16[0][0]                  \n",
      "                                                                   max_pooling1d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNor (None, 375, 128)      512         add_7[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, 375, 128)      0           batch_normalization_15[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)             (None, 375, 128)      0           activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)               (None, 375, 128)      524416      dropout_14[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 375, 128)      512         conv1d_17[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, 375, 128)      0           batch_normalization_16[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)             (None, 375, 128)      0           activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)               (None, 375, 128)      524416      dropout_15[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)   (None, 375, 128)      0           add_7[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_8 (Add)                      (None, 375, 128)      0           conv1d_18[0][0]                  \n",
      "                                                                   max_pooling1d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)               (None, 375, 192)      786624      add_8[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNor (None, 375, 192)      768         conv1d_19[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_17 (Activation)       (None, 375, 192)      0           batch_normalization_17[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)             (None, 375, 192)      0           activation_17[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)               (None, 188, 192)      1179840     dropout_16[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNor (None, 188, 192)      768         conv1d_20[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, 188, 192)      0           batch_normalization_18[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)             (None, 188, 192)      0           activation_18[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)               (None, 188, 192)      1179840     dropout_17[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)   (None, 188, 192)      0           conv1d_19[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_9 (Add)                      (None, 188, 192)      0           conv1d_21[0][0]                  \n",
      "                                                                   max_pooling1d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNor (None, 188, 192)      768         add_9[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_19 (Activation)       (None, 188, 192)      0           batch_normalization_19[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)             (None, 188, 192)      0           activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)               (None, 188, 192)      1179840     dropout_18[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNor (None, 188, 192)      768         conv1d_22[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_20 (Activation)       (None, 188, 192)      0           batch_normalization_20[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)             (None, 188, 192)      0           activation_20[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)               (None, 188, 192)      1179840     dropout_19[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D)  (None, 188, 192)      0           add_9[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_10 (Add)                     (None, 188, 192)      0           conv1d_23[0][0]                  \n",
      "                                                                   max_pooling1d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNor (None, 188, 192)      768         add_10[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_21 (Activation)       (None, 188, 192)      0           batch_normalization_21[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)             (None, 188, 192)      0           activation_21[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)               (None, 94, 192)       1179840     dropout_20[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNor (None, 94, 192)       768         conv1d_24[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, 94, 192)       0           batch_normalization_22[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)             (None, 94, 192)       0           activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)               (None, 94, 192)       1179840     dropout_21[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D)  (None, 94, 192)       0           add_10[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "add_11 (Add)                     (None, 94, 192)       0           conv1d_25[0][0]                  \n",
      "                                                                   max_pooling1d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glob (None, 192)           0           add_11[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 13,403,648\n",
      "Trainable params: 13,398,272\n",
      "Non-trainable params: 5,376\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "directory = 'train_1008/D0_l0_multicnn=False_L23_GVP_Meta=True_kernal=32/'\n",
    "model = load_model(directory+'model.h5')\n",
    "model.layers\n",
    "new_model = Model(inputs=[model.layers[0].input], outputs=model.layers[-9].output)\n",
    "# new_model = Model(inputs=[model.layers[0].input,model.layers[1].input,model.layers[2].input,model.layers[3].input], outputs=model.layers[-11].output)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_file = np.array(d['ID'][:len(d)])\n",
    "# train_lab = np.array(d['G'][:len(d)])\n",
    "# train_lab = train_lab.astype('float32')\n",
    "\n",
    "# def validation_generation(test_file, test_lab):\n",
    "#     X5_ = []\n",
    "#     X10_ = []\n",
    "#     X25_ = []\n",
    "#     X50_ = []\n",
    "#     Y_ = []\n",
    "#     ind_ = []\n",
    "#     for i in range(test_file.shape[0]):\n",
    "#         x = X5[test_file[i]] \n",
    "#         x10 = X10[test_file[i]]\n",
    "#         x25 = X25[test_file[i]]\n",
    "#         x50 = X50[test_file[i]]\n",
    "#         X5_.append(x[random.randint(0, 4)])\n",
    "#         X10_.append(x10[random.randint(0, 4)])\n",
    "#         X25_.append(x25[random.randint(0, 4)])\n",
    "#         X50_.append(x50[random.randint(0, 4)])\n",
    "#         Y_.append(test_lab[i])\n",
    "#         ind_.append(test_file[i])\n",
    "#     X5_ = np.asarray(X5_)\n",
    "#     X10_ = np.asarray(X10_)\n",
    "#     X25_ = np.asarray(X25_)\n",
    "#     X50_ = np.asarray(X50_)\n",
    "#     Y_ = np.asarray(Y_)\n",
    "#     return X5_, X10_,X25_,X50_, Y_, ind_\n",
    "\n",
    "# X5_,X10_,X25_,X50_,Y_,ind_ = validation_generation(train_file,train_lab)\n",
    "# t = new_model.predict([X5_,X10_,X25_,X50_])\n",
    "# ind_ = pd.DataFrame(ind_, columns = ['id'])\n",
    "# t1 = pd.DataFrame(t, dtype = 'str')\n",
    "# t2 = pd.concat([ind_, t1], axis = 1)\n",
    "# t2.to_csv(directory+'flatten.csv')\n",
    "# t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%javascript\n",
    "# Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

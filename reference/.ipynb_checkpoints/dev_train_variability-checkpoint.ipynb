{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load model.py\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Conv1D, Dense, MaxPool1D, Activation, AvgPool1D,GlobalAveragePooling1D\n",
    "from keras.layers import Flatten, Add, Concatenate, Dropout, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def ResidualBlock(filters,kernel_size,strides,pool_size,inputs, l_2=0.0, activation='relu', kernel_initializer='he_normal'):\n",
    "    path1 = MaxPool1D(pool_size=pool_size, padding = 'same', strides = strides)(inputs)\n",
    "    \n",
    "    path2 = BatchNormalization()(inputs)\n",
    "    path2 = Activation(activation=activation)(path2)\n",
    "    path2 = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',\n",
    "                   kernel_regularizer = l2(l_2),\n",
    "                   kernel_initializer = kernel_initializer)(path2)\n",
    "    path2 = BatchNormalization()(path2)\n",
    "    path2 = Activation(activation=activation)(path2)\n",
    "    path2 = Conv1D(filters = filters, kernel_size = kernel_size, strides = 1, padding = 'same', \n",
    "                   kernel_regularizer = l2(l_2),\n",
    "                   kernel_initializer = kernel_initializer)(path2)\n",
    "    path2 = Add()([path2, path1])\n",
    "    return path2\n",
    "\n",
    "def build_model(length=300, n_channel=2, n_classes=2, filters=64, kernel_size=3, layers = 10,\n",
    "                activation='relu',kernel_initializer = 'he_normal', l_2=0.0):    \n",
    "    sig_inp =  Input(shape=(length, n_channel))  \n",
    "    inp = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, padding=\"same\", \n",
    "                 kernel_regularizer=l2(l_2))(sig_inp)\n",
    "    inp = BatchNormalization()(inp)\n",
    "    inp = Activation(activation=activation)(inp)\n",
    "    inp_max = MaxPool1D(pool_size=2)(inp)\n",
    "\n",
    "    l1 = Conv1D(filters=filters, kernel_size=kernel_size, strides=2, padding=\"same\",\n",
    "                kernel_regularizer=l2(l_2))(inp)\n",
    "    l1 = BatchNormalization()(l1)\n",
    "    l1 = Activation(activation=activation)(l1)\n",
    "    l1 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, padding=\"same\",\n",
    "                kernel_regularizer=l2(l_2))(l1)\n",
    "\n",
    "    new_inp = Add()([l1,inp_max])\n",
    "\n",
    "    for i in range(layers):\n",
    "    # every alternate residual block subsample its input by a factor of 2\n",
    "        if i % 2 == 1:\n",
    "            pool_size = 2\n",
    "            strides = 2\n",
    "        else:\n",
    "            pool_size = 1\n",
    "            strides = 1\n",
    "        # incremented filters    \n",
    "        if i % 4 == 3:\n",
    "            filters = 64*int(i//4 + 2)\n",
    "            new_inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = 1, padding = 'same',\n",
    "                             kernel_regularizer=l2(l_2),\n",
    "                             kernel_initializer = kernel_initializer)(new_inp)\n",
    "        new_inp = ResidualBlock(filters,kernel_size,strides,pool_size,new_inp, l_2=l_2)\n",
    "\n",
    "    new_inp = GlobalAveragePooling1D()(new_inp)\n",
    "    new_inp = BatchNormalization()(new_inp)\n",
    "    new_inp = Dense(128, kernel_regularizer=l2(l_2))(new_inp) \n",
    "    new_inp = BatchNormalization()(new_inp)\n",
    "    new_inp = Activation(activation=activation)(new_inp)\n",
    "    out = Dense(n_classes, activation='softmax', kernel_regularizer=l2(l_2))(new_inp)\n",
    "    \n",
    "    model = Model(inputs=[sig_inp],outputs=[out])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     89,
     120,
     126,
     160
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/matplotlib/__init__.py:1405: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "# %load utils.py\n",
    "import os\n",
    "import keras\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MEAN_B, STD_B = 138.712, 16.100\n",
    "MEAN_M, STD_M =  36.346, 25.224\n",
    "\n",
    "def Probability(a):\n",
    "    def prob():\n",
    "        return random.uniform(a=0, b=1) > a\n",
    "    return prob\n",
    "\n",
    "def DA_RandomNoise(x):\n",
    "    noise = np.array([[random.gauss(mu=0, sigma=0.01), \n",
    "                       random.gauss(mu=0, sigma=0.01)] for _ in range(x.shape[0])], dtype=np.float32)\n",
    "    return x + noise\n",
    "\n",
    "def DA_Shift(x, smin=-5, smax=5):\n",
    "    shift = np.around(random.uniform(a=smin, b=smax))\n",
    "    return x + shift\n",
    "\n",
    "def DA_Scale(x, smin=0.8, smax=1.2):\n",
    "    scale = random.uniform(a=smin, b=smax)\n",
    "    return np.round(x*scale)\n",
    "\n",
    "def DA_TimeWarp(X, sigma=0.2):\n",
    "    length = X.shape[0]\n",
    "    channel = X.shape[1]\n",
    "    knot = 4\n",
    "    \n",
    "    from scipy.interpolate import CubicSpline      # for warping\n",
    "\n",
    "    xx = (np.ones((channel,1))*(np.arange(0, length, (length-1)/(knot+1)))).transpose()\n",
    "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, channel))\n",
    "    x_range = np.arange(length)\n",
    "    cs_x = CubicSpline(xx[:,0], yy[:,0])\n",
    "    tt = np.array([cs_x(x_range)]).transpose()\n",
    "\n",
    "    tt_new = np.cumsum(tt, axis=0)        # Add intervals to make a cumulative graph\n",
    "    # Make the last value to have X.shape[0]\n",
    "    t_scale = [(length-1)/tt_new[-1,0]]\n",
    "    tt_new[:,0] = tt_new[:,0]*t_scale[0]\n",
    "\n",
    "    # tt_new = DistortTimesteps(X, sigma)\n",
    "    X_new = np.zeros(X.shape)\n",
    "    x_range = np.arange(length)\n",
    "    \n",
    "    X_new[:,0] = np.interp(x_range, tt_new[:,0], X[:,0])\n",
    "    X_new[:,1] = np.interp(x_range, tt_new[:,0], X[:,1])\n",
    "    return X_new\n",
    "\n",
    "def data_preprocess(x,aug_func=[], prob=0.5, normalized=True):\n",
    "    do_or_not = Probability(prob)\n",
    "    \n",
    "    length = x.shape[0]\n",
    "    # get x and then remove zeros (no info)\n",
    "    x = x[(x[:,0] > 0.0) * (x[:,1] > 0.0)]\n",
    "    \n",
    "    # add random_noise\n",
    "    if aug_func:\n",
    "        for func in aug_func:\n",
    "            if do_or_not():\n",
    "                x = func(x)\n",
    "    \n",
    "    if normalized:\n",
    "        x[:,0] = (x[:,0] - MEAN_B)/STD_B\n",
    "        x[:,1] = (x[:,1] - MEAN_M)/STD_M\n",
    "\n",
    "        \n",
    "        # x1, x2 = np.mean(x, axis=0)\n",
    "#         noise = np.array([[random.gauss(mu=0, sigma=0.01), \n",
    "#                            random.gauss(mu=0, sigma=0.01)] for _ in range(x.shape[0])], dtype=np.float32)\n",
    "#         x = x + noise\n",
    "\n",
    "    # transpose to (n_channel, arbitrary length), then padd to (n_channel, length)\n",
    "    x = pad_sequences(np.transpose(x), padding='post', value=0.0, maxlen=length, dtype=np.float)\n",
    "\n",
    "    # transpose back to original shape and store\n",
    "    return np.transpose(x)\n",
    "\n",
    "\n",
    "def k_slice_X(Xvalid, Yvalid, k_slice=5, length=300, class_weight = {}):\n",
    "    \"\"\"\n",
    "    # moving across a sequence, we slice out \"k_slice\" segments with a constant interval\n",
    "    # in order to increase validation data\n",
    "    # ex:  |------------------|\n",
    "    # 1    |------|\n",
    "    # 2       |------|\n",
    "    # 3          |------|\n",
    "    # 4             |------|\n",
    "    # 5                |------|\n",
    "    \"\"\"\n",
    "    if not class_weight:\n",
    "        class_weight = dict()\n",
    "        for i in range(Yvalid.shape[1]):\n",
    "            class_weight[i] = 1\n",
    "\n",
    "    intvl = (Xvalid.shape[1] - length)//k_slice\n",
    "\n",
    "    Xtest = np.empty((Xvalid.shape[0]*k_slice, length, Xvalid.shape[2]))\n",
    "    Ytest = np.empty((Yvalid.shape[0]*k_slice, Yvalid.shape[1]))\n",
    "    Wtest = np.empty((Yvalid.shape[0]*k_slice,))\n",
    "\n",
    "    for k in range(k_slice):\n",
    "        st = k * Xvalid.shape[0]\n",
    "        for i in range(Xvalid.shape[0]):\n",
    "            # print(st+i)\n",
    "            Xtest[st+i,:,:] = data_preprocess(Xvalid[i,k*intvl:(k*intvl+length),:])\n",
    "            Ytest[st+i,:] = Yvalid[i,:]\n",
    "            Wtest[st+i] = class_weight[np.argmax(Yvalid[i,:])]\n",
    "    return Xtest, Ytest, Wtest\n",
    "\n",
    "def get_n_zeros(d):\n",
    "    n_zeros = list()\n",
    "    for i in range(d.shape[0]):\n",
    "        n_zeros.append(sum(d[i,:] ==0))\n",
    "    return np.array(n_zeros)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot_keras_csv_logger(csv_logger, save_dir='', accuracy=False):\n",
    "    loss = pd.read_table(csv_logger.filename, delimiter=',')\n",
    "    print('min val_loss {0} at epoch {1}'.format(min(loss.val_loss), np.argmin(loss.val_loss)))\n",
    "    plt.plot(loss.epoch, loss.loss, label='loss')\n",
    "    plt.plot(loss.epoch, loss.val_loss, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.savefig(os.path.join(save_dir, 'loss.png'))\n",
    "    plt.close()\n",
    "\n",
    "    if accuracy:\n",
    "        print('max val_accu {0} at epoch {1}'.format(max(loss.val_acc), np.argmax(loss.val_acc)))\n",
    "        plt.plot(loss.epoch, loss.acc, label='accu')\n",
    "        plt.plot(loss.epoch, loss.val_acc, label='val_accu')\n",
    "        plt.legend()\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.savefig(os.path.join(save_dir, 'accu.png'))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"/data/put_data/cmchang/gynecology/data/\"\n",
    "d = pd.read_csv(os.path.join(data_dir, 'data_merged.csv'))\n",
    "\n",
    "# ['variability', 'deceleration', 'management', 'UA']\n",
    "\n",
    "FLAG_target = 'variability'\n",
    "n_classes = len(set(d[FLAG_target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acceptable_zeros_threshold = 90\n",
    "d = d[get_n_zeros(np.array(d[[k for k in d.columns if 'b-' in k]], dtype=np.float)) <= acceptable_zeros_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b-599\r"
     ]
    }
   ],
   "source": [
    "for k in d.columns:\n",
    "    if 'b-' in k or 'm-' in k:\n",
    "        print(k, end='\\r')\n",
    "        d.loc[d[k]==0, k] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_id, valid_id = train_test_split(list(set(d.ID)), test_size=0.3, random_state=13)\n",
    "\n",
    "train_d, valid_d = d[[k in set(train_id) for k in d.ID]], d[[k in set(valid_id) for k in d.ID]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_db = np.array(train_d[[k for k in train_d.columns if 'b-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "train_dm = np.array(train_d[[k for k in train_d.columns if 'm-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "\n",
    "valid_db = np.array(valid_d[[k for k in valid_d.columns if 'b-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "valid_dm = np.array(valid_d[[k for k in valid_d.columns if 'm-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain = np.stack([train_db, train_dm], axis=2)\n",
    "Xvalid = np.stack([valid_db, valid_dm], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ytrain = keras.utils.to_categorical(np.array(train_d[FLAG_target]), num_classes=n_classes)\n",
    "Yvalid = keras.utils.to_categorical(np.array(valid_d[FLAG_target]), num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_rep = 5\n",
    "# aug_Xtrain = np.empty((Xtrain.shape[0] * n_rep, Xtrain.shape[1], Xtrain.shape[2]), dtype=np.float32)\n",
    "# for i in range(Xtrain.shape[0]):\n",
    "#     for j in range(n_rep):\n",
    "#         aug_Xtrain[i*n_rep+j, :, : ] = DA_TimeWarp(Xtrain[i,:,:], sigma=0.2)\n",
    "        \n",
    "# aug_Ytrain = np.repeat(Ytrain, repeats=n_rep, axis=0)\n",
    "\n",
    "# aug_Xtrain = np.concatenate([Xtrain, aug_Xtrain])\n",
    "# aug_Ytrain = np.concatenate([Ytrain, aug_Ytrain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolution\n",
    "kernel_size = 3\n",
    "filters = 64\n",
    "strides = 1\n",
    "layers = 10\n",
    "activation='relu'\n",
    "kernel_initializer = 'RandomNormal'\n",
    "\n",
    "# input, output\n",
    "FLAG_batch_size = 32\n",
    "FLAG_length = 300\n",
    "FLAG_n_channel = 2\n",
    "\n",
    "# Training\n",
    "lr_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.5694444444444444, 1: 4.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "y_integers = np.argmax(Ytrain, axis=1)\n",
    "d_class_weight = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "class_weight = dict(enumerate(d_class_weight))\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class_weight = {0: 1.0, 1: #4.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# class_weight = dict()\n",
    "# for k, v in dict(Counter(train_d[FLAG_target])).items():\n",
    "#     class_weight[k] = min(train_d.shape[0]//v, 25)\n",
    "# print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (123, 600, 2), valid: (270, 300, 2)\n"
     ]
    }
   ],
   "source": [
    "# moving across a sequence, we slice out \"k_slice\" segments with a constant interval\n",
    "# in order to increase validation data\n",
    "\"\"\"\n",
    "# ex:  |------------------|\n",
    "# 1    |------|\n",
    "# 2       |------|\n",
    "# 3          |------|\n",
    "# 4             |------|\n",
    "# 5                |------|\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "k_slice = 5\n",
    "intvl = (Xvalid.shape[1] - FLAG_length)//5\n",
    "\n",
    "Xtest = np.empty((Xvalid.shape[0]*k_slice, FLAG_length, Xvalid.shape[2]))\n",
    "Ytest = np.empty((Yvalid.shape[0]*k_slice, Yvalid.shape[1]))\n",
    "Wtest = np.empty((Yvalid.shape[0]*k_slice,))\n",
    "\n",
    "for k in range(k_slice):\n",
    "    st = k * Xvalid.shape[0]\n",
    "    for i in range(Xvalid.shape[0]):\n",
    "        # print(st+i)\n",
    "        Xtest[st+i,:,:] = data_preprocess(Xvalid[i,k*intvl:(k*intvl+FLAG_length),:])\n",
    "        Ytest[st+i,:] = Yvalid[i,:]\n",
    "        Wtest[st+i] = class_weight[np.argmax(Yvalid[i,:])]\n",
    "    \n",
    "print('train: {0}, valid: {1}'.format(Xtrain.shape, Xtest.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory /data/put_data/cmchang/gynecology/model/variability_test already exists.\n"
     ]
    }
   ],
   "source": [
    "print(\"===== create directory =====\")\n",
    "model_id = FLAG.target + \"_\" + datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "model_save = os.path.join(FLAG.model_save, FLAG.target, model_id)\n",
    "summary_save = os.path.join(FLAG.model_save, FLAG.target, 'summary_'+FLAG.target+'.csv')\n",
    "\n",
    "if not os.path.exists(model_save):\n",
    "    os.makedirs(model_save)\n",
    "    print(model_save)\n",
    "\n",
    "\n",
    "FLAG_model_save = os.path.join('/data/put_data/cmchang/gynecology/model/', FLAG_target+'_test')\n",
    "if not os.path.exists(FLAG_model_save):\n",
    "    os.mkdir(FLAG_model_save)\n",
    "    print('directory {0} is created.'.format(FLAG_model_save))\n",
    "else:\n",
    "    print('directory {0} already exists.'.format(FLAG_model_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_generator(Xtrain, Ytrain, length=300, n_channel=2, n_classes=2, aug_func=[], prob=0.5, batch_size=16):\n",
    "    n_sample = Xtrain.shape[0]\n",
    "    n_length = Xtrain.shape[1]\n",
    "    ind = list(range(n_sample))\n",
    "    x = np.empty((batch_size, length, n_channel), dtype=np.float)\n",
    "    y = np.empty((batch_size, n_classes), dtype=int)\n",
    "\n",
    "    while True:\n",
    "        np.random.shuffle(ind)\n",
    "        for i in range(n_sample//batch_size):\n",
    "            st = random.choice(np.arange(0, Xtrain.shape[1] - length))\n",
    "            i_batch = ind[i*batch_size:(i+1)*batch_size]\n",
    "            for j, k in enumerate(i_batch):\n",
    "                x[j,:] = data_preprocess(Xtrain[k,st:(st+length),:], aug_func=aug_func, prob=prob)\n",
    "                y[j,:] = Ytrain[k,:]\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.optimizers import Adam, SGD, Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_model(length=FLAG_length, n_channel=FLAG_n_channel, n_classes=n_classes, filters=filters, kernel_size=kernel_size, layers=layers,\n",
    "                   activation=activation, kernel_initializer=kernel_initializer, l_2=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adamax(lr=lr_rate, beta_1=0.5, beta_2=0.999, epsilon=1e-08, decay = 0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare DataGenerator for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_logger = keras.callbacks.CSVLogger(os.path.join(FLAG_model_save, 'training.log'))\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(os.path.join(FLAG_model_save, 'model.h5'), \n",
    "                                             monitor='val_loss', \n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=False,\n",
    "                                             mode='min',\n",
    "                                             period=1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor = 0.5, patience = 5, min_lr = 0, cooldown = 5, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', patience=30, min_delta=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model on dataset\n",
    "model.fit_generator(generator=my_generator(Xtrain, Ytrain, \n",
    "                                           #aug_func=[DA_Shift, DA_Scale],\n",
    "                                           length=FLAG_length, \n",
    "                                           n_channel=FLAG_n_channel, \n",
    "                                           n_classes=n_classes,\n",
    "                                           #prob=0.25\n",
    "                                          ),\n",
    "                    class_weight=class_weight,\n",
    "                    validation_data=(Xtest, Ytest, Wtest),\n",
    "                    steps_per_epoch=50, \n",
    "                    epochs=150,\n",
    "                    callbacks=[csv_logger,\n",
    "                               reduce_lr, \n",
    "                               checkpoint,\n",
    "                              earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = pd.read_table(csv_logger.filename, delimiter=',')\n",
    "\n",
    "plt.plot(loss.epoch, loss.loss, label='loss')\n",
    "plt.plot(loss.epoch, loss.val_loss, label='val_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss (cross entropy)')\n",
    "# plt.xlim([1,50])\n",
    "# plt.ylim([0,2])\n",
    "plt.savefig(os.path.join(FLAG_model_save, 'loss.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = pd.read_table(csv_logger.filename, delimiter=',')\n",
    "\n",
    "plt.plot(loss.epoch, loss.acc, label='accu')\n",
    "plt.plot(loss.epoch, loss.val_acc, label='val_accu')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig(os.path.join(FLAG_model_save, 'accu.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "trained_model = load_model(os.path.join(FLAG_model_save,'model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Pred = trained_model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred_aug = np.argmax(Pred,axis=1)\n",
    "ytest_aug = np.argmax(Ytest,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred = (np.mean(ypred_aug.reshape(5,-1), axis=0) > 0.5) + 0 # voting\n",
    "# ypred = np.argmax(np.mean(ypred_aug.reshape(5, 84, 2), axis=0), axis=1)\n",
    "ytest = np.argmax(Yvalid, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(np.mean(ypred_aug.reshape(5,-1), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter(np.mean(ypred_aug.reshape(5,-1), axis=0)[ypred != ytest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save wrong prediction plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(FLAG_model_save, 'wrong/')):\n",
    "    os.mkdir(os.path.join(FLAG_model_save, 'wrong/'))\n",
    "    print('{0} created'.format(os.path.join(FLAG_model_save, 'wrong/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mom  = valid_dm[ypred != ytest]\n",
    "baby = valid_db[ypred != ytest]\n",
    "\n",
    "for i in range(mom.shape[0]):\n",
    "    key = np.array(valid_d.key)[(ypred != ytest)][i]\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(mom[i,:], '.-', color='red')\n",
    "    plt.ylabel('Mom')\n",
    "    plt.title('pred={0}, true={1}'.format( ypred[(ypred != ytest)][i], ytest[(ypred != ytest)][i]))\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(baby[i,:], '.-')\n",
    "    plt.xlabel('time (s)')\n",
    "    plt.ylabel('Baby')\n",
    "    plt.title('key={0}'.format(key))\n",
    "\n",
    "    plt.savefig(os.path.join(FLAG_model_save, 'wrong/'+key+'.png'))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mom  = valid_dm[(ypred + ytest) == 2]\n",
    "baby = valid_db[(ypred + ytest) == 2]\n",
    "\n",
    "for i in range(mom.shape[0]):\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(mom[i,:], '.-', color='red')\n",
    "    plt.ylabel('Mom')\n",
    "    plt.title('n_zeros = {0}'.format(get_n_zeros(mom[i:(i+1),:])[0]))\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(baby[i,:], '.-')\n",
    "    plt.xlabel('time (s)')\n",
    "    plt.ylabel('Baby')\n",
    "    plt.title('n_zeros = {0}'.format(get_n_zeros(baby[i:(i+1),:])[0]))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # plt.savefig(os.path.join(data_dir, '../eda/'+key[i]+'.png'))type=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_pred=ypred, y_true=ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_pred=ypred_aug, y_true=ytest_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(y_pred=ypred, y_true=ytest)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cfm, classes=np.arange(n_classes),\n",
    "                      title='Confusion matrix, without normalization')\n",
    "plt.savefig(os.path.join(FLAG_model_save, 'voting_confusion_matrix.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(y_pred=ypred_aug, y_true=ytest_aug)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cfm, classes=np.arange(n_classes),\n",
    "                      title='Confusion matrix, without normalization')\n",
    "plt.savefig(os.path.join(FLAG_model_save, 'segment_confusion_matrix.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_name = ''\n",
    "for l in trained_model.layers:\n",
    "    if 'global_average_pooling' in l.name:\n",
    "        feature_name = l.name\n",
    "if not feature_name:\n",
    "    print('global_average_pooling layer not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = trained_model.get_layer(feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extractor = Model(inputs=trained_model.input, outputs=features.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_feature_list = list()\n",
    "valid_feature_list.append(extractor.predict(x=Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Fvalid = np.array(valid_feature_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "F_embedded = TSNE(n_components=2, perplexity=100).fit_transform(Fvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.argmax(Ytest,axis=1)\n",
    "color = ['#e41a1c','#377eb8','#4daf4a','#984ea3','#ff7f00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F_embedded\n",
    "for l in (set(labels)):\n",
    "    plt.scatter(F_embedded[list(np.where(labels==l)[0]),0], F_embedded[list(np.where(labels==l)[0]),1],\n",
    "                color=color[l], alpha=0.8)\n",
    "plt.xlabel('tSNE-0')\n",
    "plt.ylabel('tSNE-1')\n",
    "legend=plt.legend(title='Altitude', fontsize=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import skimage.io as io \n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "from func import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#### parser\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-d' ,'--data', type=str, default='/home/katieyth/gynecology/data/data_cmu_ctu.csv', help='data')\n",
    "# parser.add_argument('-s' ,'--model_save', type=str, default='/home/katieyth/gynecology/model_save/', help='model save path')\n",
    "parser.add_argument('-y' ,'--target', type=str, default='multi', help='prediction target')\n",
    "# # variability\tUA\t deceleration management\n",
    "\n",
    "# # input parameter\n",
    "# parser.add_argument('-th','--acceptable_zeros_threshold', type=float, default=200, help='acceptable number of missing values in raw data')\n",
    "# parser.add_argument('-l' ,'--length', type=int, default=600, help='length of input')\n",
    "# parser.add_argument('-ks','--k_slice', type=int, default=1, help='a input will be sliced into k_slice segments when testing')\n",
    "# parser.add_argument('-c' ,'--n_channel', type=int, default=2, help='number of input channels')\n",
    "# parser.add_argument('-rn','--random_noise', type=int, default=0, help='add Gaussian noise (mean=0, std=0.01) into inputs')\n",
    "# parser.add_argument('-nm','--normalized', type=int, default=1, help='whether conduct channel-wise normalization')\n",
    "# parser.add_argument('-fb' ,'--force_binary', type=int, default=0, help='force to binary task')\n",
    "parser.add_argument('-ctu_cmu' ,'--ctu_cmu', type=str, default='cmu', help='train_ctu_test_cmu')\n",
    "parser.add_argument('-multi_task' ,'--multi_task', type=int, default=1, help='multi-task')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # data augmentation \n",
    "# parser.add_argument('-aug_fliplr' ,'--aug_fliplr', type=int, default=0, help='reverse time series')\n",
    "# parser.add_argument('-shift' ,'--DA_Shift', type=int, default=1, help='')\n",
    "# parser.add_argument('-scale' ,'--DA_Scale', type=int, default=1, help='')\n",
    "# parser.add_argument('-randsamp' ,'--DA_RandSampling', type=int, default=1, help='')\n",
    "\n",
    "\n",
    "# # model parameters\n",
    "# parser.add_argument('-struc' ,'--struc', type=str, default='deeper', help='deeper or shallower')\n",
    "# parser.add_argument('-k' ,'--kernel_size', type=int, default=3, help='kernel size')\n",
    "# parser.add_argument('-f' ,'--filters', type=int, default=64, help='base number of filters')\n",
    "# parser.add_argument('-ly' ,'--layers', type=int, default=10, help='number of residual layers')\n",
    "# parser.add_argument('-a' ,'--activation', type=str, default='relu', help='activation function')\n",
    "# parser.add_argument('-i' ,'--kernel_initializer', type=str, default='RandomNormal', help='kernel initialization method')\n",
    "# parser.add_argument('-l2','--l2', type=float, default=0.01, help='coefficient of l2 regularization')\n",
    "\n",
    "# # hyper-parameters\n",
    "# parser.add_argument('-lr','--learning_rate', type=float, default=1e-4, help='learning_rate')\n",
    "# parser.add_argument('-reduce_lr_patience','--reduce_lr_patience', type=int, default=50, help='reduce_lr_patience')\n",
    "# parser.add_argument('-bs','--batch_size', type=int, default=16, help='batch_size')\n",
    "# parser.add_argument('-ep','--epoch', type=int, default=1500, help='epoch')\n",
    "parser.add_argument('-wb','--weight_balance', type=int, default=0, help='whether weight balancing or not')\n",
    "# parser.add_argument('-mntr','--monitor', type=str, default='val_man_acc', help='val_acc or val_loss')\n",
    "\n",
    "\n",
    "# parser.add_argument('-g' ,'--gpu_id', type=str, default='7', help='GPU ID')\n",
    "parser.add_argument('-rs' ,'--random_state', type=int, default=13, help='random state when train_test_split')\n",
    "# parser.add_argument('-fn' ,'--summary_file', type=str, default=None, help='summary filename')\n",
    "\n",
    "\n",
    "FLAG = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess_test(Xvalid, Yvalid,  length=600, class_weight = {}):\n",
    "\n",
    "    Xtest = np.empty((Xvalid.shape[0], length, Xvalid.shape[2]))\n",
    "    for i in range(Xvalid.shape[0]):\n",
    "        # print(st+i)\n",
    "        Xtest[i,:,:] = data_normalize(Xvalid[i,0:600,:])\n",
    "    if not class_weight:\n",
    "        class_weight = []\n",
    "        for c in range(4):\n",
    "            weight = dict()\n",
    "            for i in range(Yvalid[c].shape[1]):\n",
    "                weight[i] = 1\n",
    "            class_weight.append(weight)\n",
    "    Ytest = Yvalid\n",
    "    return Xtest, Ytest, class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (8,10,11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m-599\r"
     ]
    }
   ],
   "source": [
    "d = pd.read_csv(os.path.join(FLAG.data))\n",
    "# d = d[myutils.get_n_zeros(np.array(d[[k for k in d.columns if 'b-' in k]], dtype=np.float)) <= FLAG.acceptable_zeros_threshold]\n",
    "\n",
    "# if FLAG.force_binary : \n",
    "#     d[d[FLAG.target]>1] = 1\n",
    "# n_classes = 4#len(set(d[FLAG.target]))\n",
    "\n",
    "# replace 0 (no readings) with np.nan for later substitution\n",
    "for k in d.columns:\n",
    "    if 'b-' in k or 'm-' in k:\n",
    "        print(k, end='\\r')\n",
    "        d.loc[d[k]==0, k] = np.nan\n",
    "\n",
    "if FLAG.ctu_cmu == 'trans':\n",
    "    train_d = d[d['ID'].str.contains('CTU_')]\n",
    "    valid_d = d[d['ID'].str.contains('CMU_')]\n",
    "elif FLAG.ctu_cmu == 'mix':\n",
    "    train_d,valid_d = train_test_split(d, test_size=0.3, random_state=FLAG.random_state, stratify =d['management'])\n",
    "elif FLAG.ctu_cmu == 'cmu':\n",
    "    dd = d[d['ID'].str.contains('CMU_')]\n",
    "    train_d,valid_d = train_test_split(dd, test_size=0.3, random_state=FLAG.random_state, stratify =dd['management'])\n",
    "elif FLAG.ctu_cmu == 'trans_inv':\n",
    "    train_d = d[d['ID'].str.contains('CMU_')]\n",
    "    valid_d = d[d['ID'].str.contains('CTU_')]\n",
    "    \n",
    "    \n",
    "# interpolate missing values\n",
    "train_db = np.array(train_d[[k for k in train_d.columns if 'b-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "train_dm = np.array(train_d[[k for k in train_d.columns if 'm-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "\n",
    "valid_db = np.array(valid_d[[k for k in valid_d.columns if 'b-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "valid_dm = np.array(valid_d[[k for k in valid_d.columns if 'm-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "\n",
    "# combine signals from baby and mom\n",
    "Xtrain = np.stack([train_db, train_dm], axis=2)\n",
    "Xvalid = np.stack([valid_db, valid_dm], axis=2)\n",
    "\n",
    "# convert labels to one-hot encodings\n",
    "if FLAG.multi_task:\n",
    "    Ytrain_man = keras.utils.to_categorical(np.array(train_d['management']),   num_classes=3)#len(set(np.array(train_d['management']))))\n",
    "    Ytrain_ua  = keras.utils.to_categorical(np.array(train_d['UA']),           num_classes=2)#len(set(np.array(train_d['UA']))))\n",
    "    Ytrain_var = keras.utils.to_categorical(np.array(train_d['variability']),  num_classes=2)#len(set(np.array(train_d['variability']))))\n",
    "    Ytrain_dec = keras.utils.to_categorical(np.array(train_d['deceleration']), num_classes=4)#len(set(np.array(train_d['deceleration']))))\n",
    "    Ytrain = [Ytrain_man, Ytrain_ua, Ytrain_var, Ytrain_dec]\n",
    "\n",
    "    Yvalid_man = keras.utils.to_categorical(np.array(valid_d['management']),   num_classes=3)#len(set(np.array(train_d['management']))))\n",
    "    Yvalid_ua  = keras.utils.to_categorical(np.array(valid_d['UA']),           num_classes=2)#len(set(np.array(train_d['UA']))))\n",
    "    Yvalid_var = keras.utils.to_categorical(np.array(valid_d['variability']),  num_classes=2)#len(set(np.array(train_d['variability']))))\n",
    "    Yvalid_dec = keras.utils.to_categorical(np.array(valid_d['deceleration']), num_classes=4)#len(set(np.array(train_d['deceleration']))))\n",
    "    Yvalid = [Yvalid_man, Yvalid_ua, Yvalid_var, Yvalid_dec]\n",
    "    #weight balancing or not\n",
    "    if FLAG.weight_balance:\n",
    "        weight_list = []\n",
    "        for i in range(4):\n",
    "            y_integers = np.argmax(Ytrain[i], axis=1)\n",
    "            d_class_weight = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "            class_weight = dict(enumerate(d_class_weight))\n",
    "            print('class weight: {0}'.format(class_weight))\n",
    "            weight_list.append(class_weight)\n",
    "    else:\n",
    "        weight_list = []\n",
    "        \n",
    "        for c in [3,2,2,4]:\n",
    "            class_weight = dict()\n",
    "            for i in range(c):\n",
    "                class_weight[i] = 1\n",
    "            weight_list.append(class_weight)\n",
    "\n",
    "else:\n",
    "    Ytrain = keras.utils.to_categorical(np.array(train_d[FLAG.target]), num_classes=n_classes)\n",
    "    Yvalid = keras.utils.to_categorical(np.array(valid_d[FLAG.target]), num_classes=n_classes)\n",
    "    #weight balancing or not\n",
    "    if FLAG.weight_balance:\n",
    "\n",
    "        y_integers = np.argmax(Ytrain, axis=1)\n",
    "        d_class_weight = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "        class_weight = dict(enumerate(d_class_weight))\n",
    "        print('class weight: {0}'.format(class_weight))\n",
    "    else:\n",
    "        class_weight = dict()\n",
    "        for i in range(n_classes):\n",
    "            class_weight[i] = 1\n",
    "\n",
    "\n",
    "\n",
    "Xtest, Ytest, Wtest =data_preprocess_test(Xvalid, Yvalid, class_weight = weight_list)\n",
    "#print(Wtest)\n",
    "# print(Xtest.shape)\n",
    "# print(Xtest[0:5])\n",
    "# class weight: {0: 0.5892575039494471, 1: 1.0626780626780628, 2: 2.762962962962963}\n",
    "# class weight: {0: 0.6411764705882353, 1: 1.09, 2: 1.912280701754386}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain, Wtest =data_preprocess_test(Xtrain, Ytrain, class_weight = weight_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Define func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get gradient\n",
    "def get_grad(model, model_pred, tg_position, grad_layer):\n",
    "    '''get_grad(model = trained_model,\n",
    "                model_pred = pred,\n",
    "                tg_position = tg_dict[tg],\n",
    "                grad_layer = 'conv1d_25') '''\n",
    "    prd_cls = np.argmax(pred[tg_position], axis = 1)\n",
    "    # 只有一個dim，所以取0那個dim，再指定 model 預測的 class (不是 G-true label)\n",
    "    y_c = model.output[tg_position][0,prd_cls]\n",
    "    # 選最後一個 conv layer 作為要取 gradient 的位置\n",
    "    conv_layer = model.get_layer(grad_layer).output\n",
    "    # tf.gradients()\n",
    "    grads = K.gradients(y_c, conv_layer)[0]\n",
    "    gradient_function = K.function([model.input], [conv_layer, grads])\n",
    "    ###### flow to the graph and get *target layer and * corresponding gradient\n",
    "    conv_output, grads_val = gradient_function([ctg_x])\n",
    "    \n",
    "    return conv_output, grads_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# follow paper method (Grad-CAM)\n",
    "def GCAM(feature_map, Fmap_grad):\n",
    "    '''GCAM(feature_map = conv_output,\n",
    "            Fmap_grad = grads_val)'''\n",
    "    ### (1,10,192) feature map size: (1,10) feature map num: 192 \n",
    "    ### get mean gradients for each feature map as its weight\n",
    "    weights = np.mean(Fmap_grad, axis = (0, 1))\n",
    "    ### multiply each F_map by its corresponding weight\n",
    "    for i, w in enumerate(weights):  \n",
    "            feature_map[:, :, i] *= w \n",
    "    ### sum each feature point by point to get 'global weight' (1:10) \n",
    "    hightlight_fmap = np.sum(feature_map, axis=2)\n",
    "    ### passing relu, we only concern those with positive reponse to model's prediction\n",
    "    hightlight_fmap_relu = np.maximum(hightlight_fmap, 0) \n",
    "    \n",
    "    \n",
    "    return hightlight_fmap_relu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get material for visualizing 1D Grad-CAM\n",
    "def get_material_vis_by_tg(model, tg_position, pred):\n",
    "    ''' get_material_vis_by_tg(model = trained_model,\n",
    "                               tg_position = tg_dict[tg],\n",
    "                               pred = pred)'''\n",
    "    conv_output, grads_val = get_grad(model = model,\n",
    "                                  model_pred = pred,\n",
    "                                  tg_position = tg_position,\n",
    "                                  grad_layer = 'conv1d_25')\n",
    "    hightlight_fmap_relu = GCAM(feature_map = conv_output,\n",
    "                                Fmap_grad = grads_val)\n",
    "#     HL_extend = np.repeat(hightlight_fmap_relu, 60)\n",
    "#     HL_extend = np.expand_dims(HL_extend, axis = 0)\n",
    "    x_axis = [i for i in range(600)]\n",
    "    \n",
    "    return x_axis, hightlight_fmap_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create grid: 4 2-pairs\n",
    "def create_grid():\n",
    "    n = 4 # number of double-rows\n",
    "    m = 1 # number of columns\n",
    "\n",
    "    t = 0.9 # 1-t == top space \n",
    "    b = 0.1 # bottom space      (both in figure coordinates)\n",
    "\n",
    "    msp = 0.1 # minor spacing\n",
    "    sp = 0.5  # major spacing\n",
    "\n",
    "    offs=(1+msp)*(t-b)/(2*n+n*msp+(n-1)*sp) # grid offset\n",
    "    hspace = sp+msp+1 #height space per grid\n",
    "\n",
    "    gso = GridSpec(n,m, bottom=b+offs, top=t, hspace=hspace)\n",
    "    gse = GridSpec(n,m, bottom=b, top=t-offs, hspace=hspace)\n",
    "\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    axes1 = []\n",
    "    axes2 = []\n",
    "    for i in range(n*m):\n",
    "        axes1.append(fig.add_subplot(gso[i]))\n",
    "        axes2.append(fig.add_subplot(gse[i]))\n",
    "    return fig, axes1, axes2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visulize 1D Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_dict = {'man':0,\n",
    "          'ua':1,\n",
    "          'var':2,\n",
    "          'dec':3}\n",
    "tg_list = ['Management',\n",
    "          'UA',\n",
    "          'Variability',\n",
    "          'Deceleration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pstn = 'model_save/multi/multi_190104161240/' \n",
    "model_path = os.path.join('/home/katieyth/gynecology/', model_pstn)\n",
    "trained_model = load_model(os.path.join(model_path,'model.h5'))\n",
    "\n",
    "\n",
    "deal_Xset = Xtest\n",
    "deal_Yset = Ytest\n",
    "deal_ID = valid_d.ID\n",
    "save_dir = os.path.join(model_pstn, 'GCAM_1D/test_conv25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kk = np.expand_dims(hightlight_fmap_relu, axis=0)\n",
    "# plt.imshow(kk, cmap = 'coolwarm')\n",
    "# plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81    CMU_363194_67380\n",
       "Name: ID, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deal_ID[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAABECAYAAABpjjW9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAXxJREFUeJzt26FNpVEURtHzCIKCEDh6gGAoAYF8\nBUwJ0wRB0wAhYDG0MBZFMmJC8lMByah3SfZa+opP7Rxzd9u2DQAtR6sHAHB44g8QJP4AQeIPECT+\nAEHiDxAk/gBB4g8QJP4AQcerB3zn79P98q/Hz1e/Vk+YmZl/75+rJ8zj79fVE2ZmZv/nZvWEObm4\nXj1hZmaezm5XT5jTt7vVE2Zm5nL/sXrCj/HycL77n3cuf4Ag8QcIEn+AIPEHCBJ/gCDxBwgSf4Ag\n8QcIEn+AIPEHCBJ/gCDxBwgSf4Ag8QcIEn+AIPEHCBJ/gCDxBwgSf4Ag8QcIEn+AIPEHCBJ/gCDx\nBwgSf4Ag8QcIEn+AIPEHCBJ/gCDxBwgSf4Ag8QcIEn+AoN22bas3AHBgLn+AIPEHCBJ/gCDxBwgS\nf4Ag8QcIEn+AIPEHCBJ/gCDxBwgSf4Ag8QcIEn+AIPEHCBJ/gCDxBwgSf4Ag8QcIEn+AIPEHCBJ/\ngCDxBwgSf4CgLzM0GYPwkkvYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1faf08bc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAABECAYAAABpjjW9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAXpJREFUeJzt26FJhmEUhuHziwhiNTuECxiNgt1t\n3MAlBPOPRbNgMDuBCxgNn8EsmHzDfV0TPOnmlLPbtm0AaDlYPQCA/yf+AEHiDxAk/gBB4g8QJP4A\nQeIPECT+AEHiDxB0uHrAb/ZvX8tfj9/ed6snzMzM5fnn6gmzfz1ZPWFmZp4eXlZPmPuzu9UTZmbm\n9Ppq9YR5vrxdPWFmZj4e31dPmOOj1Qt+3Fzs/hQulz9AkPgDBIk/QJD4AwSJP0CQ+AMEiT9AkPgD\nBIk/QJD4AwSJP0CQ+AMEiT9AkPgDBIk/QJD4AwSJP0CQ+AMEiT9AkPgDBIk/QJD4AwSJP0CQ+AME\niT9AkPgDBIk/QJD4AwSJP0CQ+AMEiT9AkPgDBIk/QNBu27bVGwD4Zy5/gCDxBwgSf4Ag8QcIEn+A\nIPEHCBJ/gCDxBwgSf4Ag8QcIEn+AIPEHCBJ/gCDxBwgSf4Ag8QcIEn+AIPEHCBJ/gCDxBwgSf4Ag\n8QcIEn+AoG+wFReDWxNRYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1fd1c439b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAABECAYAAABpjjW9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAXtJREFUeJzt26Eux1Ech+Hv32gaN+AaNMUtcT26\nEU1VBMVmo+mCLIk/V2CTnPA+Tz7hE87enXJ227YNAC17qwcA8P/EHyBI/AGCxB8gSPwBgsQfIEj8\nAYLEHyBI/AGC9lcP+M3n++vyr8eHD9erJ8zMzOPF3eoJc/ZytXrCzMwcfH+tnjAfx6erJ8zMzMnr\nzeoJc3t0uXrCzMw8v6y/F2+Pb6snzMzM0/357i/nvPwBgsQfIEj8AYLEHyBI/AGCxB8gSPwBgsQf\nIEj8AYLEHyBI/AGCxB8gSPwBgsQfIEj8AYLEHyBI/AGCxB8gSPwBgsQfIEj8AYLEHyBI/AGCxB8g\nSPwBgsQfIEj8AYLEHyBI/AGCxB8gSPwBgsQfIEj8AYJ227at3gDAP/PyBwgSf4Ag8QcIEn+AIPEH\nCBJ/gCDxBwgSf4Ag8QcIEn+AIPEHCBJ/gCDxBwgSf4Ag8QcIEn+AIPEHCBJ/gCDxBwgSf4Ag8QcI\nEn+AIPEHCPoB11Qbg4VHWAYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1fbab14fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAABECAYAAABpjjW9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAASxJREFUeJzt27ENAjEQRUGM6IqACqj2KiCgrqUC\nJEjOwZuJHfzoaROvmbkA0HLdPQCA84k/QJD4AwSJP0CQ+AMEiT9AkPgDBIk/QJD4AwTddg/45v58\n+XoM8Kf38Vi/vHP5AwSJP0CQ+AMEiT9AkPgDBIk/QJD4AwSJP0CQ+AMEiT9AkPgDBIk/QJD4AwSJ\nP0CQ+AMEiT9AkPgDBIk/QJD4AwSJP0CQ+AMEiT9AkPgDBIk/QJD4AwSJP0CQ+AMEiT9AkPgDBIk/\nQJD4AwSJP0CQ+AMErZnZvQGAk7n8AYLEHyBI/AGCxB8gSPwBgsQfIEj8AYLEHyBI/AGCxB8gSPwB\ngsQfIEj8AYLEHyBI/AGCxB8gSPwBgsQfIEj8AYLEHyBI/AGCxB8gSPwBgj7B2AqDC1oh8gAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1fb8013710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/63\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in float_scalars\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:54: RuntimeWarning: invalid value encountered in float_scalars\n"
     ]
    }
   ],
   "source": [
    "for idx, ID in enumerate(deal_ID[0:1]):\n",
    "    ctg_x = np.expand_dims(deal_Xset[idx], axis=0)\n",
    "\n",
    "    ctg_y_man = np.argmax(deal_Yset[0][idx])\n",
    "    ctg_y_ua  = np.argmax(deal_Yset[1][idx])\n",
    "    ctg_y_var = np.argmax(deal_Yset[2][idx])\n",
    "    ctg_y_dec = np.argmax(deal_Yset[3][idx])\n",
    "    ctg_y = [deal_Yset[0][idx],\n",
    "             deal_Yset[1][idx],\n",
    "             deal_Yset[2][idx],\n",
    "             deal_Yset[3][idx]]\n",
    "    pred = trained_model.predict(ctg_x)\n",
    "    \n",
    "    fig, axes1, axes2 = create_grid()\n",
    "    for od, tg_name in enumerate(['man', 'ua', 'var', 'dec']):\n",
    "        x_axis, hightlight_fmap_relu \\\n",
    "        = get_material_vis_by_tg(model = trained_model,\n",
    "                                 tg_position = tg_dict[tg_name],\n",
    "                                 pred = pred)\n",
    "        plt.close(fig)\n",
    "        kk = np.expand_dims(hightlight_fmap_relu, axis=0)\n",
    "        plt.imshow(kk, cmap = 'coolwarm')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        pred_prob = [int(v*100) for v in pred[od][0]]\n",
    "        true_prob = [int(v) for v in ctg_y[od]]\n",
    "\n",
    "\n",
    "        ax = axes1[od]\n",
    "        for i in range(len(hightlight_fmap_relu)):\n",
    "            local_max = max(hightlight_fmap_relu) # temporary, should get global max in the future (?)\n",
    "            ax.plot(x_axis[i*60:(i+1)*60],\n",
    "                     ctg_x[0][i*60:(i+1)*60,0], # baby\n",
    "                     color = cm.coolwarm(hightlight_fmap_relu[i]/local_max))\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "        if tg_name == 'man':\n",
    "            ax.set_title('◉  '+tg_list[od] + '    #ID: %s' %(ID), x = 0, loc = 'left')\n",
    "        else:\n",
    "            ax.set_title('◉  '+tg_list[od], x = 0, loc = 'left')\n",
    "        \n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "        ax.set_ylabel('True Label\\n  %s\\n'%(true_prob)+\\\n",
    "                      'Model Confidence (%)'+'\\n  %s' %(pred_prob),\n",
    "                      fontsize=10, rotation = 0,\n",
    "                      horizontalalignment='left',\n",
    "                      position=(0,1))\n",
    "\n",
    "        ax = axes2[od]\n",
    "        for i in range(len(hightlight_fmap_relu)):\n",
    "            local_max = max(hightlight_fmap_relu) # temporary, should get global max in the future (?)\n",
    "            ax.plot(x_axis[i*60:(i+1)*60],\n",
    "                    ctg_x[0][i*60:(i+1)*60,1], # mommy\n",
    "                    color = cm.coolwarm(hightlight_fmap_relu[i]/local_max))\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "    check_dir(save_dir)\n",
    "#     fig.savefig(os.path.join(save_dir, '%s.png' % (ID)), dpi=150, bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "    print( '%d/%d' %(idx, len(deal_Xset)), end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# annotate color bar ( Not REQUIRED )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deal_p = os.listdir(save_dir)\n",
    "c_bar = io.imread('./Screenshot from 2019-01-09 15-03-40.png')\n",
    "for idx, fname in enumerate(deal_p):\n",
    "    fig2 = plt.figure(figsize=(10,6), dpi = 200)\n",
    "    outer_grid = GridSpec(2,1, height_ratios=[1, 20],\n",
    "                         hspace=0)\n",
    "    ax0 = fig2.add_subplot(outer_grid[0])\n",
    "\n",
    "    ax0.annotate('⇦ Lower Response', xy=(200,23), xytext=(20, 23), size = 8, color= 'white')\n",
    "    ax0.annotate('Higher Response ⇨', xy=(200,23), xytext=(400, 23), size = 8, color= 'white')\n",
    "    ax0.imshow(c_bar[:31,:,:])\n",
    "    ax0.axis('off')\n",
    "\n",
    "    ax1 = fig2.add_subplot(outer_grid[1])\n",
    "    p_signal = io.imread(os.path.join(save_dir, fname))\n",
    "    ax1.imshow(p_signal[10:,10:,:])\n",
    "    ax1.axis('off')\n",
    "    fig2.savefig(os.path.join(save_dir, fname), dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig2)\n",
    "    print( '%d/%d' %(idx, len(deal_p)), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

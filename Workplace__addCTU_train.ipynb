{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import keras\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.optimizers import Adam, SGD, Adamax\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from model import build_model\n",
    "from func import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#### parser\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-d' ,'--data', type=str, default='/home/katieyth/gynecology/data/data_merged.csv', help='data')\n",
    "parser.add_argument('-s' ,'--model_save', type=str, default='/home/katieyth/gynecology/model_save/', help='model save path')\n",
    "parser.add_argument('-y' ,'--target', type=str, default='management', help='prediction target')\n",
    "# variability\tUA\t deceleration management\n",
    "\n",
    "# input parameter\n",
    "parser.add_argument('-th','--acceptable_zeros_threshold', type=float, default=200, help='acceptable number of missing values in raw data')\n",
    "parser.add_argument('-l' ,'--length', type=int, default=600, help='length of input')\n",
    "parser.add_argument('-ks','--k_slice', type=int, default=1, help='a input will be sliced into k_slice segments when testing')\n",
    "parser.add_argument('-c' ,'--n_channel', type=int, default=2, help='number of input channels')\n",
    "parser.add_argument('-rn','--random_noise', type=int, default=0, help='add Gaussian noise (mean=0, std=0.01) into inputs')\n",
    "parser.add_argument('-nm','--normalized', type=int, default=1, help='whether conduct channel-wise normalization')\n",
    "parser.add_argument('-fb' ,'--force_binary', type=int, default=0, help='force to binary task')\n",
    "parser.add_argument('-ctu_cmu' ,'--ctu_cmu', type=int, default=1, help='train_ctu_test_cmu')\n",
    "parser.add_argument('-multi_task' ,'--multi_task', type=int, default=1, help='multi-task')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data augmentation \n",
    "parser.add_argument('-aug_fliplr' ,'--aug_fliplr', type=int, default=0, help='reverse time series')\n",
    "parser.add_argument('-shift' ,'--DA_Shift', type=int, default=1, help='')\n",
    "parser.add_argument('-scale' ,'--DA_Scale', type=int, default=1, help='')\n",
    "parser.add_argument('-randsamp' ,'--DA_RandSampling', type=int, default=1, help='')\n",
    "\n",
    "\n",
    "# model parameters\n",
    "parser.add_argument('-k' ,'--kernel_size', type=int, default=3, help='kernel size')\n",
    "parser.add_argument('-f' ,'--filters', type=int, default=64, help='base number of filters')\n",
    "parser.add_argument('-ly' ,'--layers', type=int, default=10, help='number of residual layers')\n",
    "parser.add_argument('-a' ,'--activation', type=str, default='relu', help='activation function')\n",
    "parser.add_argument('-i' ,'--kernel_initializer', type=str, default='RandomNormal', help='kernel initialization method')\n",
    "parser.add_argument('-l2','--l2', type=float, default=0.01, help='coefficient of l2 regularization')\n",
    "\n",
    "# hyper-parameters\n",
    "parser.add_argument('-lr','--learning_rate', type=float, default=1e-4, help='learning_rate')\n",
    "parser.add_argument('-reduce_lr_patience','--reduce_lr_patience', type=int, default=5, help='reduce_lr_patience')\n",
    "parser.add_argument('-bs','--batch_size', type=int, default=16, help='batch_size')\n",
    "parser.add_argument('-ep','--epoch', type=int, default=1000, help='epoch')\n",
    "parser.add_argument('-wb','--weight_balance', type=int, default=0, help='whether weight balancing or not')\n",
    "parser.add_argument('-mntr','--monitor', type=str, default='val_acc', help='val_acc or val_loss')\n",
    "\n",
    "\n",
    "parser.add_argument('-g' ,'--gpu_id', type=str, default='7', help='GPU ID')\n",
    "parser.add_argument('-rs' ,'--random_state', type=int, default=13, help='random state when train_test_split')\n",
    "parser.add_argument('-fn' ,'--summary_file', type=str, default=None, help='summary filename')\n",
    "\n",
    "\n",
    "FLAG = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== train =====\n",
      "b-599\r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (4,4) into shape (4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-54dc66d1dacc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWtest\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mdata_preprocess_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;31m#print(Wtest)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# print(Xtest.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gynecology/func.py\u001b[0m in \u001b[0;36mdata_preprocess_test\u001b[0;34m(Xvalid, Yvalid, length, class_weight)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# print(st+i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mXtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXvalid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mYtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYvalid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mWtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYvalid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (4,4) into shape (4)"
     ]
    }
   ],
   "source": [
    "print(\"===== train =====\")\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = FLAG.gpu_id\n",
    "d = pd.read_csv(os.path.join(FLAG.data))\n",
    "# d = d[myutils.get_n_zeros(np.array(d[[k for k in d.columns if 'b-' in k]], dtype=np.float)) <= FLAG.acceptable_zeros_threshold]\n",
    "\n",
    "if FLAG.force_binary : \n",
    "    d[d[FLAG.target]>1] = 1\n",
    "n_classes = len(set(d[FLAG.target]))\n",
    "\n",
    "# replace 0 (no readings) with np.nan for later substitution\n",
    "for k in d.columns:\n",
    "    if 'b-' in k or 'm-' in k:\n",
    "        print(k, end='\\r')\n",
    "        d.loc[d[k]==0, k] = np.nan\n",
    "\n",
    "if FLAG.ctu_cmu:\n",
    "    train_d = d[d['ID'].str.contains('CTU_')]\n",
    "    valid_d = d[d['ID'].str.contains('CMU_')]\n",
    "else:\n",
    "    train_d,valid_d = train_test_split(d, test_size=0.3, random_state=FLAG.random_state, stratify =d[FLAG.target])\n",
    "\n",
    "\n",
    "\n",
    "# interpolate missing values\n",
    "train_db = np.array(train_d[[k for k in train_d.columns if 'b-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "train_dm = np.array(train_d[[k for k in train_d.columns if 'm-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "\n",
    "valid_db = np.array(valid_d[[k for k in valid_d.columns if 'b-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "valid_dm = np.array(valid_d[[k for k in valid_d.columns if 'm-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "\n",
    "# combine signals from baby and mom\n",
    "Xtrain = np.stack([train_db, train_dm], axis=2)\n",
    "Xvalid = np.stack([valid_db, valid_dm], axis=2)\n",
    "\n",
    "# convert labels to one-hot encodings\n",
    "if FLAG.multi_task:\n",
    "    Ytrain_man = keras.utils.to_categorical(np.array(train_d['management']),   num_classes=4)#len(set(np.array(train_d['management']))))\n",
    "    Ytrain_ua  = keras.utils.to_categorical(np.array(train_d['UA']),           num_classes=4)#len(set(np.array(train_d['UA']))))\n",
    "    Ytrain_var = keras.utils.to_categorical(np.array(train_d['variability']),  num_classes=4)#len(set(np.array(train_d['variability']))))\n",
    "    Ytrain_dec = keras.utils.to_categorical(np.array(train_d['deceleration']), num_classes=4)#len(set(np.array(train_d['deceleration']))))\n",
    "    Ytrain = np.stack([Ytrain_man, Ytrain_ua, Ytrain_var, Ytrain_dec], axis = 2)\n",
    "\n",
    "    Yvalid_man = keras.utils.to_categorical(np.array(valid_d['management']),   num_classes=4)#len(set(np.array(train_d['management']))))\n",
    "    Yvalid_ua  = keras.utils.to_categorical(np.array(valid_d['UA']),           num_classes=4)#len(set(np.array(train_d['UA']))))\n",
    "    Yvalid_var = keras.utils.to_categorical(np.array(valid_d['variability']),  num_classes=4)#len(set(np.array(train_d['variability']))))\n",
    "    Yvalid_dec = keras.utils.to_categorical(np.array(valid_d['deceleration']), num_classes=4)#len(set(np.array(train_d['deceleration']))))\n",
    "    Yvalid = np.stack([Yvalid_man, Yvalid_ua, Yvalid_var, Yvalid_dec], axis = 2)\n",
    "    \n",
    "else:\n",
    "    Ytrain = keras.utils.to_categorical(np.array(train_d[FLAG.target]), num_classes=n_classes)\n",
    "    Yvalid = keras.utils.to_categorical(np.array(valid_d[FLAG.target]), num_classes=n_classes)\n",
    "\n",
    "# weight balancing or not\n",
    "if FLAG.weight_balance:\n",
    "\n",
    "    y_integers = np.argmax(Ytrain, axis=1)\n",
    "    d_class_weight = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "    class_weight = dict(enumerate(d_class_weight))\n",
    "    print('class weight: {0}'.format(class_weight))\n",
    "else:\n",
    "    class_weight = dict()\n",
    "    for i in range(n_classes):\n",
    "        class_weight[i] = 1\n",
    "\n",
    "Xtest, Ytest, Wtest =data_preprocess_test(Xvalid, Yvalid, class_weight = class_weight)\n",
    "#print(Wtest)\n",
    "# print(Xtest.shape)\n",
    "# print(Xtest[0:5])\n",
    "# class weight: {0: 0.5892575039494471, 1: 1.0626780626780628, 2: 2.762962962962963}\n",
    "# class weight: {0: 0.6411764705882353, 1: 1.09, 2: 1.912280701754386}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG.aug_fliplr:\n",
    "    Xtrain_copy = Xtrain.copy()\n",
    "    for i in range(len(Xtrain)):\n",
    "        Xtrain_copy[i] = np.fliplr([Xtrain[i]])[0]\n",
    "#         plt.plot(Xtrain[i])\n",
    "#         plt.show()\n",
    "#         plt.plot(Xtrain_copy[i])\n",
    "#         plt.show()\n",
    "        print(i,'/',len(Xtrain), end= '\\r')\n",
    "    Xtrain = np.vstack((Xtrain, Xtrain_copy))\n",
    "    Ytrain = np.vstack((Ytrain, Ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augtool = (DA_Shift,DA_Scale,DA_RandSampling)\n",
    "choose_augtool = (FLAG.DA_Shift,FLAG.DA_Scale,FLAG.DA_RandSampling)\n",
    "augset = [x for x, y in zip(augtool, choose_augtool) if y == 1]\n",
    "augset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = my_generator(Xtrain, Ytrain, \n",
    "                        length=FLAG.length, \n",
    "                        n_channel=FLAG.n_channel, \n",
    "                        n_classes=n_classes,\n",
    "                        random_noise=FLAG.random_noise,\n",
    "                        normalized=FLAG.normalized,\n",
    "                        batch_size=FLAG.batch_size,\n",
    "                        aug_func=augset,\n",
    "                        prob=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== create directory =====\")\n",
    "model_id = FLAG.target + \"_\" + datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "model_save = os.path.join(FLAG.model_save, FLAG.target, model_id)\n",
    "summary_save = os.path.join(FLAG.model_save, FLAG.target, 'summary_'+FLAG.target+'.csv')\n",
    "\n",
    "if not os.path.exists(model_save):\n",
    "    os.makedirs(model_save)\n",
    "    print(model_save)\n",
    "\n",
    "# declare model \n",
    "model = build_model(length=FLAG.length, n_channel=FLAG.n_channel, n_classes=n_classes, filters=FLAG.filters, kernel_size=FLAG.kernel_size, layers=FLAG.layers,\n",
    "                activation=FLAG.activation, kernel_initializer=FLAG.kernel_initializer, l_2=FLAG.l2)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "lr_rate = FLAG.learning_rate\n",
    "adam = Adamax(lr_rate, beta_1=0.5, beta_2=0.999, epsilon=1e-08, decay = 0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "csv_logger = keras.callbacks.CSVLogger(os.path.join(model_save, 'training.log'))\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(os.path.join(model_save, 'model.h5'), \n",
    "                                            monitor=FLAG.monitor, \n",
    "                                            verbose=1, \n",
    "                                            save_best_only=True,\n",
    "                                            save_weights_only=False,\n",
    "                                            mode='auto',\n",
    "                                            period=1)\n",
    "earlystop = EarlyStopping(monitor = FLAG.monitor, patience=20, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=FLAG.monitor, factor = 0.5, patience = FLAG.reduce_lr_patience, min_lr = 0, cooldown = 5, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(generator=train_gen,\n",
    "                    class_weight=class_weight,\n",
    "                    validation_data=(Xtest, Ytest),#, Wtest),\n",
    "                    steps_per_epoch=50, \n",
    "                    epochs=10, #FLAG.epoch,\n",
    "                    verbose=1,\n",
    "                    callbacks=[csv_logger,\n",
    "                            #reduce_lr, \n",
    "                            checkpoint,\n",
    "                            #earlystop\n",
    "                              ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keras_csv_logger(csv_logger, save_dir=model_save, accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = model_save\n",
    "print(path)\n",
    "# path = '/home/katieyth/gynecology/model_save/management/management_181224174216'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate validation set\n",
    "trained_model = load_model(os.path.join(path,'model.h5'))\n",
    "Pred = trained_model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_hd_l = np.argmax(Pred, axis=1)\n",
    "# pred_dataset = valid_d['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = trained_model.get_layer('global_average_pooling1d_1')\n",
    "# extractor = Model(inputs=trained_model.input, outputs=features.output)\n",
    "# Fout = extractor.predict(x=Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O_X = valid_d[FLAG.target] == pred_hd_l\n",
    "# print(len(O_X)-sum(O_X))\n",
    "# print(sum(O_X)/len(O_X))\n",
    "# O_X[O_X==1] = 'gray'\n",
    "# O_X[O_X==0] = 'red'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_emb = pca.fit_transform(Fout)\n",
    "# num = len([l for l in valid_d['ID'] if 'CMU_' in l])\n",
    "# fig = plt.figure(figsize=(15,3))\n",
    "# plt.subplot('13%s' % (1))\n",
    "# plt.scatter(pca_emb[:num,0], pca_emb[:num,1], c = valid_d[FLAG.target][:num], marker='o', cmap='coolwarm', alpha=0.4)\n",
    "# plt.scatter(pca_emb[num:,0], pca_emb[num:,1], c = valid_d[FLAG.target][num:], marker='X', cmap='coolwarm', alpha=0.4)\n",
    "# plt.ylabel('%s\\n PCA (dim=2)' %(FLAG.target), rotation=65)\n",
    "# plt.title('GT')\n",
    "# plt.subplot('13%s' % (2))\n",
    "# plt.scatter(pca_emb[:num,0], pca_emb[:num,1], c = pred_hd_l[:num], marker='o', cmap='coolwarm', alpha=0.4)\n",
    "# plt.scatter(pca_emb[num:,0], pca_emb[num:,1], c = pred_hd_l[num:], marker='X', cmap='coolwarm', alpha=0.4)\n",
    "# plt.title('Pred')\n",
    "# plt.subplot('13%s' % (3))\n",
    "# plt.scatter(pca_emb[:num,0], pca_emb[:num,1], c = O_X[:num], marker='o', alpha=0.4)\n",
    "# plt.scatter(pca_emb[num:,0], pca_emb[num:,1], c = O_X[num:], marker='X', alpha=0.4)\n",
    "# plt.title('Wrong Prediction')\n",
    "# plt.show()\n",
    "\n",
    "# check_dir('./plot/feature_analysis/')\n",
    "# fig.savefig('./plot/feature_analysis/F_analysis_%s.png' % (FLAG.target), dpi=100, format='png',bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# evaluate by every segment\n",
    "ypred_aug = np.argmax(Pred , axis=1)\n",
    "ytest_aug = np.argmax(Ytest, axis=1)\n",
    "\n",
    "cfm = confusion_matrix(y_pred=ypred_aug, y_true=ytest_aug)\n",
    "recall = np.diag(cfm) / np.sum(cfm, axis=1)\n",
    "precision = np.diag(cfm) / np.sum(cfm, axis=0)\n",
    "val_accu = accuracy_score(y_pred=ypred_aug, y_true=ytest_aug)\n",
    "\n",
    "print('accuracy : ', val_accu)\n",
    "for i in range(n_classes):\n",
    "    print('recall-{0} : {1}'.format(i, recall[i]))\n",
    "#     print('precision-{0}: {1}'.format(i, precision[i]))\n",
    "for i in range(n_classes):\n",
    "#     print('recall-{0} : {1}'.format(i, recall[i]))\n",
    "    print('precision-{0}: {1}'.format(i, precision[i]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "rcl = [round(recall[i],2) for i in range(n_classes)]\n",
    "plot_confusion_matrix(cfm, classes=np.arange(n_classes), title='%s acc: %.2f \\nrecall: %s' %(FLAG.target, val_accu,\n",
    "                                                                                rcl),\n",
    "                     save_dir=model_save)\n",
    "# plt.close()\n",
    "# plt.savefig('./plot/feature_analysis/cfm_%s.png' % (FLAG.target), dpi=100, format='png',bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read traing.log\n",
    "loss = pd.read_table(csv_logger.filename, delimiter=',')\n",
    "best_val_loss = np.min(loss.val_loss)\n",
    "best_epoch = np.argmin(loss.val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save into dictionary\n",
    "sav = vars(FLAG)\n",
    "sav['epoch'] = best_epoch\n",
    "sav['val_loss'] = best_val_loss\n",
    "sav['val_accu'] = val_accu\n",
    "sav['model_id'] = model_id\n",
    "\n",
    "for i in range(n_classes):\n",
    "    sav['recall-{0}'.format(i)] = recall[i]\n",
    "    sav['precision-{0}'.format(i)] = precision[i]\n",
    "\n",
    "# append into summary files\n",
    "dnew = pd.DataFrame(sav, index=[0])\n",
    "if os.path.exists(summary_save):\n",
    "    dori = pd.read_csv(summary_save)\n",
    "    dori = pd.concat([dori, dnew])\n",
    "    dori.to_csv(summary_save, index=False)\n",
    "else:\n",
    "    dnew.to_csv(summary_save, index=False)\n",
    "\n",
    "print(summary_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/katieyth/gynecology/utils.py:8: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-94144c8f89dc>\", line 12, in <module>\n",
      "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2095, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<decorator-gen-107>\", line 2, in matplotlib\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/magic.py\", line 187, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/magics/pylab.py\", line 99, in matplotlib\n",
      "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2978, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/pylabtools.py\", line 308, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/matplotlib/pyplot.py\", line 229, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/usr/lib/python3.5/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('Agg')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import keras\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.optimizers import Adam, SGD, Adamax\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import utils as myutils\n",
    "from model import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#### parser\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-d' ,'--data_dir', type=str, default='./data/', help='data directory')\n",
    "parser.add_argument('-s' ,'--model_save', type=str, default='/home/katieyth/gynecology/model_save/', help='model save path')\n",
    "parser.add_argument('-y' ,'--target', type=str, default='management', help='prediction target')\n",
    "# variability\tUA\t deceleration management\n",
    "\n",
    "# input parameter\n",
    "parser.add_argument('-th','--acceptable_zeros_threshold', type=float, default=200, help='acceptable number of missing values in raw data')\n",
    "parser.add_argument('-l' ,'--length', type=int, default=600, help='length of input')\n",
    "parser.add_argument('-ks','--k_slice', type=int, default=1, help='a input will be sliced into k_slice segments when testing')\n",
    "parser.add_argument('-c' ,'--n_channel', type=int, default=2, help='number of input channels')\n",
    "parser.add_argument('-rn','--random_noise', type=int, default=0, help='add Gaussian noise (mean=0, std=0.01) into inputs')\n",
    "parser.add_argument('-nm','--normalized', type=int, default=1, help='whether conduct channel-wise normalization')\n",
    "parser.add_argument('-fb' ,'--force_binary', type=int, default=0, help='force to binary task')\n",
    "parser.add_argument('-sb' ,'--split_by_patient', type=int, default=1, help='split by patient')\n",
    "\n",
    "# data augmentation \n",
    "parser.add_argument('-aug_fliplr' ,'--aug_fliplr', type=int, default=1, help='reverse time series')\n",
    "parser.add_argument('-shift' ,'--DA_Shift', type=int, default=0, help='')\n",
    "parser.add_argument('-scale' ,'--DA_Scale', type=int, default=0, help='')\n",
    "parser.add_argument('-randsamp' ,'--DA_RandSampling', type=int, default=0, help='')\n",
    "\n",
    "\n",
    "# model parameters\n",
    "parser.add_argument('-k' ,'--kernel_size', type=int, default=3, help='kernel size')\n",
    "parser.add_argument('-f' ,'--filters', type=int, default=64, help='base number of filters')\n",
    "parser.add_argument('-ly' ,'--layers', type=int, default=10, help='number of residual layers')\n",
    "parser.add_argument('-a' ,'--activation', type=str, default='relu', help='activation function')\n",
    "parser.add_argument('-i' ,'--kernel_initializer', type=str, default='RandomNormal', help='kernel initialization method')\n",
    "parser.add_argument('-l2','--l2', type=float, default=0.1, help='coefficient of l2 regularization')\n",
    "\n",
    "# hyper-parameters\n",
    "parser.add_argument('-lr','--learning_rate', type=float, default=1e-3, help='learning_rate')\n",
    "parser.add_argument('-reduce_lr_patience','--reduce_lr_patience', type=int, default=5, help='reduce_lr_patience')\n",
    "parser.add_argument('-bs','--batch_size', type=int, default=16, help='batch_size')\n",
    "parser.add_argument('-ep','--epoch', type=int, default=1000, help='epoch')\n",
    "parser.add_argument('-wb','--weight_balance', type=int, default=1, help='whether weight balancing or not')\n",
    "parser.add_argument('-mntr','--monitor', type=str, default='val_acc', help='val_acc or val_loss')\n",
    "\n",
    "\n",
    "parser.add_argument('-g' ,'--gpu_id', type=str, default='1', help='GPU ID')\n",
    "parser.add_argument('-rs' ,'--random_state', type=int, default=13, help='random state when train_test_split')\n",
    "parser.add_argument('-fn' ,'--summary_file', type=str, default=None, help='summary filename')\n",
    "\n",
    "\n",
    "FLAG = parser.parse_args([])\n",
    "\n",
    "#train(FLAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     59,
     67
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:9: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-94144c8f89dc>\", line 12, in <module>\n",
      "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2095, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<decorator-gen-107>\", line 2, in matplotlib\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/magic.py\", line 187, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/magics/pylab.py\", line 99, in matplotlib\n",
      "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2978, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/pylabtools.py\", line 308, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/matplotlib/pyplot.py\", line 229, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/usr/lib/python3.5/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# %load utils.py\n",
    "import os\n",
    "import keras\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MEAN_B, STD_B = 138.712, 16.100\n",
    "MEAN_M, STD_M =  36.346, 25.224\n",
    "\n",
    "def Probability(a):\n",
    "    def prob():\n",
    "        return random.uniform(a=0, b=1) > a\n",
    "    return prob\n",
    "\n",
    "def DA_RandomNoise(x):\n",
    "    noise = np.array([[random.gauss(mu=0, sigma=0.01), \n",
    "                       random.gauss(mu=0, sigma=0.01)] for _ in range(x.shape[0])], dtype=np.float32)\n",
    "    return x + noise\n",
    "\n",
    "def DA_Shift(x, smin=-5, smax=5):\n",
    "    shift = np.around(random.uniform(a=smin, b=smax))\n",
    "    return x + shift\n",
    "\n",
    "def DA_Scale(x, smin=0.8, smax=1.2):\n",
    "    scale = random.uniform(a=smin, b=smax)\n",
    "    return np.round(x*scale)\n",
    "\n",
    "def DA_TimeWarp(X, sigma=0.2):\n",
    "    length = X.shape[0]\n",
    "    channel = X.shape[1]\n",
    "    knot = 4\n",
    "    \n",
    "    from scipy.interpolate import CubicSpline      # for warping\n",
    "\n",
    "    xx = (np.ones((channel,1))*(np.arange(0, length, (length-1)/(knot+1)))).transpose()\n",
    "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, channel))\n",
    "    x_range = np.arange(length)\n",
    "    cs_x = CubicSpline(xx[:,0], yy[:,0])\n",
    "    tt = np.array([cs_x(x_range)]).transpose()\n",
    "\n",
    "    tt_new = np.cumsum(tt, axis=0)        # Add intervals to make a cumulative graph\n",
    "    # Make the last value to have X.shape[0]\n",
    "    t_scale = [(length-1)/tt_new[-1,0]]\n",
    "    tt_new[:,0] = tt_new[:,0]*t_scale[0]\n",
    "\n",
    "    # tt_new = DistortTimesteps(X, sigma)\n",
    "    X_new = np.zeros(X.shape)\n",
    "    x_range = np.arange(length)\n",
    "    \n",
    "    X_new[:,0] = np.interp(x_range, tt_new[:,0], X[:,0])\n",
    "    X_new[:,1] = np.interp(x_range, tt_new[:,0], X[:,1])\n",
    "    return X_new\n",
    "\n",
    "def RandSampleTimesteps(X, nSample=150):\n",
    "    X_new = np.zeros(X.shape)\n",
    "    tt = np.zeros((nSample,X.shape[1]), dtype=int)\n",
    "    tt[1:-1,0] = np.sort(np.random.randint(1,X.shape[0]-1,nSample-2))\n",
    "    tt[1:-1,1] = np.sort(np.random.randint(1,X.shape[0]-1,nSample-2))\n",
    "    #tt[1:-1,2] = np.sort(np.random.randint(1,X.shape[0]-1,nSample-2))\n",
    "    tt[-1,:] = X.shape[0]-1\n",
    "    return tt\n",
    "def DA_RandSampling(X, nSample=150):  # could for UA \n",
    "    tt = RandSampleTimesteps(X, nSample)\n",
    "    X_new = np.zeros(X.shape)\n",
    "    X_new[:,0] = np.interp(np.arange(X.shape[0]), tt[:,0], X[tt[:,0],0])\n",
    "    X_new[:,1] = np.interp(np.arange(X.shape[0]), tt[:,1], X[tt[:,1],1])\n",
    "    #X_new[:,2] = np.interp(np.arange(X.shape[0]), tt[:,2], X[tt[:,2],2])\n",
    "    return X_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_preprocess(x,  normalized, random_noise, aug_func=[], prob=0.5):\n",
    "    do_or_not = Probability(prob)\n",
    "    \n",
    "    length = x.shape[0]\n",
    "    # get x and then remove zeros (no info)\n",
    "    x = x[(x[:,0] > 0.0) * (x[:,1] > 0.0)]\n",
    "    \n",
    "    # add random_noise\n",
    "    if aug_func:\n",
    "        for func in aug_func:\n",
    "            if do_or_not():\n",
    "                x = func(x)\n",
    "    \n",
    "    if normalized:\n",
    "        x[:,0] = (x[:,0] - MEAN_B)/STD_B\n",
    "        x[:,1] = (x[:,1] - MEAN_M)/STD_M\n",
    "\n",
    "    \n",
    "    if random_noise:\n",
    "        x1, x2 = np.mean(x, axis=0)\n",
    "        noise = np.array([[random.gauss(mu=0, sigma=0.01), \n",
    "                           random.gauss(mu=0, sigma=0.01)] for _ in range(x.shape[0])], dtype=np.float32)\n",
    "        x = x + noise\n",
    "\n",
    "    # transpose to (n_channel, arbitrary length), then padd to (n_channel, length)\n",
    "    x = pad_sequences(np.transpose(x), padding='post', value=0.0, maxlen=length, dtype=np.float)\n",
    "\n",
    "    # transpose back to original shape and store\n",
    "    return np.transpose(x)\n",
    "\n",
    "\n",
    "def k_slice_X(Xvalid, Yvalid, k_slice=5, length=300, class_weight = {}):\n",
    "    \"\"\"\n",
    "    # moving across a sequence, we slice out \"k_slice\" segments with a constant interval\n",
    "    # in order to increase validation data\n",
    "    # ex:  |------------------|\n",
    "    # 1    |------|\n",
    "    # 2       |------|\n",
    "    # 3          |------|\n",
    "    # 4             |------|\n",
    "    # 5                |------|\n",
    "    \"\"\"\n",
    "    if not class_weight:\n",
    "        class_weight = dict()\n",
    "        for i in range(Yvalid.shape[1]):\n",
    "            class_weight[i] = 1\n",
    "\n",
    "    intvl = (Xvalid.shape[1] - length)//k_slice\n",
    "\n",
    "    Xtest = np.empty((Xvalid.shape[0]*k_slice, length, Xvalid.shape[2]))\n",
    "    Ytest = np.empty((Yvalid.shape[0]*k_slice, Yvalid.shape[1]))\n",
    "    Wtest = np.empty((Yvalid.shape[0]*k_slice,))\n",
    "\n",
    "    for k in range(k_slice):\n",
    "        st = k * Xvalid.shape[0]\n",
    "        for i in range(Xvalid.shape[0]):\n",
    "            # print(st+i)\n",
    "            Xtest[st+i,:,:] = data_preprocess(Xvalid[i,k*intvl:(k*intvl+length),:])\n",
    "            Ytest[st+i,:] = Yvalid[i,:]\n",
    "            Wtest[st+i] = class_weight[np.argmax(Yvalid[i,:])]\n",
    "    return Xtest, Ytest, Wtest\n",
    "\n",
    "def get_n_zeros(d):\n",
    "    n_zeros = list()\n",
    "    for i in range(d.shape[0]):\n",
    "        n_zeros.append(sum(d[i,:] ==0))\n",
    "    return np.array(n_zeros)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot_keras_csv_logger(csv_logger, save_dir='', accuracy=False):\n",
    "    loss = pd.read_table(csv_logger.filename, delimiter=',')\n",
    "    print('min val_loss {0} at epoch {1}'.format(min(loss.val_loss), np.argmin(loss.val_loss)))\n",
    "    plt.plot(loss.epoch, loss.loss, label='loss')\n",
    "    plt.plot(loss.epoch, loss.val_loss, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.savefig(os.path.join(save_dir, 'loss.png'))\n",
    "    plt.close()\n",
    "\n",
    "    if accuracy:\n",
    "        print('max val_accu {0} at epoch {1}'.format(max(loss.val_acc), np.argmax(loss.val_acc)))\n",
    "        plt.plot(loss.epoch, loss.acc, label='accu')\n",
    "        plt.plot(loss.epoch, loss.val_acc, label='val_accu')\n",
    "        plt.legend()\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.savefig(os.path.join(save_dir, 'accu.png'))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== create directory =====\n",
      "/home/katieyth/gynecology/model_save/management/management_181220160318\n"
     ]
    }
   ],
   "source": [
    "print(\"===== create directory =====\")\n",
    "model_id = FLAG.target + \"_\" + datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "model_save = os.path.join(FLAG.model_save, FLAG.target, model_id)\n",
    "summary_save = os.path.join(FLAG.model_save, FLAG.target, 'summary_'+FLAG.target+'.csv')\n",
    "\n",
    "if not os.path.exists(model_save):\n",
    "    os.makedirs(model_save)\n",
    "    print(model_save)\n",
    "    \n",
    "# if not os.path.exists(model_save):\n",
    "#     os.mkdir(model_save)\n",
    "#     print('directory {0} is created.'.format(model_save))\n",
    "# else:\n",
    "#     print('directory {0} already exists.'.format(model_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== train =====\n",
      "class weight: {0: 0.5454545454545454, 1: 0.9411764705882353, 2: 9.6}\n"
     ]
    }
   ],
   "source": [
    "print(\"===== train =====\")\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = FLAG.gpu_id\n",
    "\n",
    "d = pd.read_csv(os.path.join(FLAG.data_dir, 'data_merged.csv'))\n",
    "d = d[myutils.get_n_zeros(np.array(d[[k for k in d.columns if 'b-' in k]], dtype=np.float)) <= FLAG.acceptable_zeros_threshold]\n",
    "\n",
    "if FLAG.force_binary : \n",
    "    d[d[FLAG.target]>1] = 1\n",
    "\n",
    "n_classes = len(set(d[FLAG.target]))\n",
    "\n",
    "# replace 0 (no readings) with np.nan for later substitution\n",
    "for k in d.columns:\n",
    "    if 'b-' in k or 'm-' in k:\n",
    "        print(k, end='\\r')\n",
    "        d.loc[d[k]==0, k] = np.nan\n",
    "\n",
    "# train test split\n",
    "\n",
    "if FLAG.split_by_patient:\n",
    "    train_id, valid_id = train_test_split(list(set(d.ID)), test_size=0.3, random_state=FLAG.random_state)\n",
    "    train_d, valid_d = d[[k in set(train_id) for k in d.ID]], d[[k in set(valid_id) for k in d.ID]]\n",
    "else:\n",
    "    train_d,valid_d = train_test_split(d, test_size=0.3, random_state=FLAG.random_state, stratify =d[FLAG.target])\n",
    "\n",
    "# interpolate missing values\n",
    "train_db = np.array(train_d[[k for k in train_d.columns if 'b-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "train_dm = np.array(train_d[[k for k in train_d.columns if 'm-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "\n",
    "valid_db = np.array(valid_d[[k for k in valid_d.columns if 'b-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "valid_dm = np.array(valid_d[[k for k in valid_d.columns if 'm-' in k]].interpolate(limit_direction='both', axis=1), dtype=np.float)\n",
    "\n",
    "# combine signals from baby and mom\n",
    "Xtrain = np.stack([train_db, train_dm], axis=2)\n",
    "Xvalid = np.stack([valid_db, valid_dm], axis=2)\n",
    "\n",
    "# convert labels to one-hot encodings\n",
    "Ytrain = keras.utils.to_categorical(np.array(train_d[FLAG.target]), num_classes=n_classes)\n",
    "Yvalid = keras.utils.to_categorical(np.array(valid_d[FLAG.target]), num_classes=n_classes)\n",
    "\n",
    "# weight balancing or not\n",
    "if FLAG.weight_balance:\n",
    "\n",
    "    y_integers = np.argmax(Ytrain, axis=1)\n",
    "    d_class_weight = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "    class_weight = dict(enumerate(d_class_weight))\n",
    "    print('class weight: {0}'.format(class_weight))\n",
    "else:\n",
    "    class_weight = dict()\n",
    "    for i in range(n_classes):\n",
    "        class_weight[i] = 1\n",
    "\n",
    "# k fold of validation set\n",
    "Xtest, Ytest, Wtest = myutils.k_slice_X(Xvalid, Yvalid, length=FLAG.length, k_slice=FLAG.k_slice, class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 600, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 600, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xvalid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "management\n",
       "0    88\n",
       "1    51\n",
       "2     5\n",
       "Name: management, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_d.groupby(FLAG.target)[FLAG.target].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "management\n",
       "0    45\n",
       "1    16\n",
       "2     1\n",
       "Name: management, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_d.groupby(FLAG.target)[FLAG.target].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_generator(Xtrain, Ytrain, length=300, n_channel=2, n_classes=2, batch_size=16, prob=0.5, aug_func=[], random_noise = False, normalized = True):\n",
    "    n_sample = Xtrain.shape[0]\n",
    "    n_length = Xtrain.shape[1]\n",
    "    ind = list(range(n_sample))\n",
    "    x = np.empty((batch_size, length, n_channel), dtype=np.float)\n",
    "    y = np.empty((batch_size, n_classes), dtype=int)\n",
    "\n",
    "    while True:\n",
    "        np.random.shuffle(ind)\n",
    "        for i in range(n_sample//batch_size):\n",
    "            if length==600:\n",
    "                st = 0\n",
    "            else:\n",
    "                st = random.choice(np.arange(0, Xtrain.shape[1] - length))\n",
    "            i_batch = ind[i*batch_size:(i+1)*batch_size]\n",
    "            for j, k in enumerate(i_batch):\n",
    "                x[j,:] = data_preprocess(Xtrain[k,st:(st+length),:], aug_func=aug_func, prob=prob, random_noise=random_noise, normalized=normalized)\n",
    "                y[j,:] = Ytrain[k,:]\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 600, 2)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 600, 64)      448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 600, 64)      256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 600, 64)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 300, 64)      12352       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 300, 64)      256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 300, 64)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 300, 64)      12352       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 300, 64)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 300, 64)      0           conv1d_3[0][0]                   \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 300, 64)      256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 300, 64)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 300, 64)      12352       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 300, 64)      256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 300, 64)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 300, 64)      12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 300, 64)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 300, 64)      0           conv1d_5[0][0]                   \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 300, 64)      256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 300, 64)      0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 150, 64)      12352       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 150, 64)      256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 150, 64)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 150, 64)      12352       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 150, 64)      0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 150, 64)      0           conv1d_7[0][0]                   \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 150, 64)      256         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 150, 64)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 150, 64)      12352       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 150, 64)      256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 150, 64)      0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 150, 64)      12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 150, 64)      0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 150, 64)      0           conv1d_9[0][0]                   \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 150, 128)     24704       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 150, 128)     512         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 150, 128)     0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 75, 128)      49280       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 75, 128)      512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 75, 128)      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 75, 128)      49280       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 75, 128)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 75, 128)      0           conv1d_12[0][0]                  \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 75, 128)      512         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 75, 128)      0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 75, 128)      49280       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 75, 128)      512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 75, 128)      0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 75, 128)      49280       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 75, 128)      0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 75, 128)      0           conv1d_14[0][0]                  \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 75, 128)      512         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 75, 128)      0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 38, 128)      49280       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 38, 128)      512         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 38, 128)      0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 38, 128)      49280       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 38, 128)      0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 38, 128)      0           conv1d_16[0][0]                  \n",
      "                                                                 max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 38, 128)      512         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 38, 128)      0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 38, 128)      49280       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 38, 128)      512         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 38, 128)      0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 38, 128)      49280       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 38, 128)      0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 38, 128)      0           conv1d_18[0][0]                  \n",
      "                                                                 max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 38, 192)      73920       add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 38, 192)      768         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 38, 192)      0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 19, 192)      110784      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 19, 192)      768         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 19, 192)      0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 19, 192)      110784      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 19, 192)      0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 19, 192)      0           conv1d_21[0][0]                  \n",
      "                                                                 max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 19, 192)      768         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 19, 192)      0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 19, 192)      110784      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 19, 192)      768         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 19, 192)      0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 19, 192)      110784      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 19, 192)      0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 19, 192)      0           conv1d_23[0][0]                  \n",
      "                                                                 max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 19, 192)      768         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 19, 192)      0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 10, 192)      110784      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 10, 192)      768         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 10, 192)      0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 10, 192)      110784      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 10, 192)      0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 10, 192)      0           conv1d_25[0][0]                  \n",
      "                                                                 max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 192)          0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 192)          768         global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          24704       batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 128)          0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            387         activation_23[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,293,955\n",
      "Trainable params: 1,287,939\n",
      "Non-trainable params: 6,016\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# declare model \n",
    "model = build_model(length=FLAG.length, n_channel=FLAG.n_channel, n_classes=n_classes, filters=FLAG.filters, kernel_size=FLAG.kernel_size, layers=FLAG.layers,\n",
    "                activation=FLAG.activation, kernel_initializer=FLAG.kernel_initializer, l_2=FLAG.l2)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "lr_rate = FLAG.learning_rate\n",
    "adam = Adamax(lr_rate, beta_1=0.5, beta_2=0.999, epsilon=1e-08, decay = 0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "csv_logger = keras.callbacks.CSVLogger(os.path.join(model_save, 'training.log'))\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(os.path.join(model_save, 'model.h5'), \n",
    "                                            monitor=FLAG.monitor, \n",
    "                                            verbose=1, \n",
    "                                            save_best_only=True,\n",
    "                                            save_weights_only=False,\n",
    "                                            mode='auto',\n",
    "                                            period=1)\n",
    "earlystop = EarlyStopping(monitor = FLAG.monitor, patience=20, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=FLAG.monitor, factor = 0.5, patience = FLAG.reduce_lr_patience, min_lr = 0, cooldown = 5, verbose = True)\n",
    "# clr = CyclicLR(base_lr=0.000005, max_lr=0.0001,\n",
    "#                         step_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 144\r",
      "1 / 144\r",
      "2 / 144\r",
      "3 / 144\r",
      "4 / 144\r",
      "5 / 144\r",
      "6 / 144\r",
      "7 / 144\r",
      "8 / 144\r",
      "9 / 144\r",
      "10 / 144\r",
      "11 / 144\r",
      "12 / 144\r",
      "13 / 144\r",
      "14 / 144\r",
      "15 / 144\r",
      "16 / 144\r",
      "17 / 144\r",
      "18 / 144\r",
      "19 / 144\r",
      "20 / 144\r",
      "21 / 144\r",
      "22 / 144\r",
      "23 / 144\r",
      "24 / 144\r",
      "25 / 144\r",
      "26 / 144\r",
      "27 / 144\r",
      "28 / 144\r",
      "29 / 144\r",
      "30 / 144\r",
      "31 / 144\r",
      "32 / 144\r",
      "33 / 144\r",
      "34 / 144\r",
      "35 / 144\r",
      "36 / 144\r",
      "37 / 144\r",
      "38 / 144\r",
      "39 / 144\r",
      "40 / 144\r",
      "41 / 144\r",
      "42 / 144\r",
      "43 / 144\r",
      "44 / 144\r",
      "45 / 144\r",
      "46 / 144\r",
      "47 / 144\r",
      "48 / 144\r",
      "49 / 144\r",
      "50 / 144\r",
      "51 / 144\r",
      "52 / 144\r",
      "53 / 144\r",
      "54 / 144\r",
      "55 / 144\r",
      "56 / 144\r",
      "57 / 144\r",
      "58 / 144\r",
      "59 / 144\r",
      "60 / 144\r",
      "61 / 144\r",
      "62 / 144\r",
      "63 / 144\r",
      "64 / 144\r",
      "65 / 144\r",
      "66 / 144\r",
      "67 / 144\r",
      "68 / 144\r",
      "69 / 144\r",
      "70 / 144\r",
      "71 / 144\r",
      "72 / 144\r",
      "73 / 144\r",
      "74 / 144\r",
      "75 / 144\r",
      "76 / 144\r",
      "77 / 144\r",
      "78 / 144\r",
      "79 / 144\r",
      "80 / 144\r",
      "81 / 144\r",
      "82 / 144\r",
      "83 / 144\r",
      "84 / 144\r",
      "85 / 144\r",
      "86 / 144\r",
      "87 / 144\r",
      "88 / 144\r",
      "89 / 144\r",
      "90 / 144\r",
      "91 / 144\r",
      "92 / 144\r",
      "93 / 144\r",
      "94 / 144\r",
      "95 / 144\r",
      "96 / 144\r",
      "97 / 144\r",
      "98 / 144\r",
      "99 / 144\r",
      "100 / 144\r",
      "101 / 144\r",
      "102 / 144\r",
      "103 / 144\r",
      "104 / 144\r",
      "105 / 144\r",
      "106 / 144\r",
      "107 / 144\r",
      "108 / 144\r",
      "109 / 144\r",
      "110 / 144\r",
      "111 / 144\r",
      "112 / 144\r",
      "113 / 144\r",
      "114 / 144\r",
      "115 / 144\r",
      "116 / 144\r",
      "117 / 144\r",
      "118 / 144\r",
      "119 / 144\r",
      "120 / 144\r",
      "121 / 144\r",
      "122 / 144\r",
      "123 / 144\r",
      "124 / 144\r",
      "125 / 144\r",
      "126 / 144\r",
      "127 / 144\r",
      "128 / 144\r",
      "129 / 144\r",
      "130 / 144\r",
      "131 / 144\r",
      "132 / 144\r",
      "133 / 144\r",
      "134 / 144\r",
      "135 / 144\r",
      "136 / 144\r",
      "137 / 144\r",
      "138 / 144\r",
      "139 / 144\r",
      "140 / 144\r",
      "141 / 144\r",
      "142 / 144\r",
      "143 / 144\r"
     ]
    }
   ],
   "source": [
    "if FLAG.aug_fliplr:\n",
    "    Xtrain_copy = Xtrain.copy()\n",
    "    for i in range(len(Xtrain)):\n",
    "        Xtrain_copy[i] = np.fliplr([Xtrain[i]])[0]\n",
    "#         plt.plot(Xtrain[i])\n",
    "#         plt.show()\n",
    "#         plt.plot(Xtrain_copy[i])\n",
    "#         plt.show()\n",
    "        print(i,'/',len(Xtrain), end= '\\r')\n",
    "    Xtrain = np.vstack((Xtrain, Xtrain_copy))\n",
    "    Ytrain = np.vstack((Ytrain, Ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augtool = (DA_Shift,DA_Scale,DA_RandSampling)\n",
    "choose_augtool = (FLAG.DA_Shift,FLAG.DA_Scale,FLAG.DA_RandSampling)\n",
    "augset = [x for x, y in zip(augtool, choose_augtool) if y == 1]\n",
    "augset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 354.3736 - acc: 0.5778\n",
      "Epoch 00001: val_acc improved from -inf to 0.67742, saving model to /home/katieyth/gynecology/model_save/management/management_181220160318/model.h5\n",
      "50/50 [==============================] - 20s 407ms/step - loss: 352.8644 - acc: 0.5787 - val_loss: 276.7709 - val_acc: 0.6774\n",
      "Epoch 2/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 232.6341 - acc: 0.7679\n",
      "Epoch 00002: val_acc did not improve\n",
      "50/50 [==============================] - 4s 74ms/step - loss: 231.8664 - acc: 0.7700 - val_loss: 193.2361 - val_acc: 0.6452\n",
      "Epoch 3/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 166.2748 - acc: 0.8151\n",
      "Epoch 00003: val_acc did not improve\n",
      "50/50 [==============================] - 4s 72ms/step - loss: 165.8034 - acc: 0.8150 - val_loss: 142.1097 - val_acc: 0.6452\n",
      "Epoch 4/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 125.4147 - acc: 0.8036\n",
      "Epoch 00004: val_acc did not improve\n",
      "50/50 [==============================] - 3s 65ms/step - loss: 125.1058 - acc: 0.8037 - val_loss: 109.8363 - val_acc: 0.5645\n",
      "Epoch 5/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 97.5099 - acc: 0.8661\n",
      "Epoch 00005: val_acc did not improve\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 97.2867 - acc: 0.8662 - val_loss: 86.4323 - val_acc: 0.5323\n",
      "Epoch 6/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 77.5618 - acc: 0.8265\n",
      "Epoch 00006: val_acc did not improve\n",
      "50/50 [==============================] - 4s 80ms/step - loss: 77.4083 - acc: 0.8263 - val_loss: 70.0990 - val_acc: 0.4032\n",
      "Epoch 7/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 63.3506 - acc: 0.8520\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "50/50 [==============================] - 4s 90ms/step - loss: 63.2302 - acc: 0.8525 - val_loss: 57.6920 - val_acc: 0.5000\n",
      "Epoch 8/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 54.5132 - acc: 0.9375\n",
      "Epoch 00008: val_acc did not improve\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 54.4614 - acc: 0.9375 - val_loss: 52.4601 - val_acc: 0.5161\n",
      "Epoch 9/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 49.4710 - acc: 0.9375\n",
      "Epoch 00009: val_acc did not improve\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 49.4301 - acc: 0.9338 - val_loss: 47.6480 - val_acc: 0.5484\n",
      "Epoch 10/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 44.8279 - acc: 0.9617\n",
      "Epoch 00010: val_acc did not improve\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 44.7843 - acc: 0.9613 - val_loss: 43.2510 - val_acc: 0.5968\n",
      "Epoch 11/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 40.7297 - acc: 0.9094\n",
      "Epoch 00011: val_acc did not improve\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 40.6891 - acc: 0.9100 - val_loss: 39.2778 - val_acc: 0.6613\n",
      "Epoch 12/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 36.9471 - acc: 0.9209\n",
      "Epoch 00012: val_acc did not improve\n",
      "50/50 [==============================] - 4s 80ms/step - loss: 36.9103 - acc: 0.9225 - val_loss: 35.7434 - val_acc: 0.6129\n",
      "Epoch 13/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 33.4560 - acc: 0.9413\n",
      "Epoch 00013: val_acc did not improve\n",
      "50/50 [==============================] - 4s 70ms/step - loss: 33.4222 - acc: 0.9425 - val_loss: 32.5143 - val_acc: 0.5000\n",
      "Epoch 14/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 30.6371 - acc: 0.9477\n",
      "Epoch 00014: val_acc did not improve\n",
      "50/50 [==============================] - 4s 79ms/step - loss: 30.6123 - acc: 0.9475 - val_loss: 30.2830 - val_acc: 0.6129\n",
      "Epoch 15/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 28.3107 - acc: 0.9490\n",
      "Epoch 00015: val_acc did not improve\n",
      "50/50 [==============================] - 4s 72ms/step - loss: 28.2901 - acc: 0.9487 - val_loss: 28.1649 - val_acc: 0.4839\n",
      "Epoch 16/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 26.1804 - acc: 0.9643\n",
      "Epoch 00016: val_acc did not improve\n",
      "50/50 [==============================] - 3s 70ms/step - loss: 26.1601 - acc: 0.9650 - val_loss: 26.0063 - val_acc: 0.5806\n",
      "Epoch 17/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 24.2371 - acc: 0.9745\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00017: val_acc did not improve\n",
      "50/50 [==============================] - 4s 72ms/step - loss: 24.2240 - acc: 0.9700 - val_loss: 24.2732 - val_acc: 0.5806\n",
      "Epoch 18/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 23.0500 - acc: 0.9490\n",
      "Epoch 00018: val_acc did not improve\n",
      "50/50 [==============================] - 4s 72ms/step - loss: 23.0410 - acc: 0.9500 - val_loss: 23.8213 - val_acc: 0.5645\n",
      "Epoch 19/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 22.3365 - acc: 0.9783\n",
      "Epoch 00019: val_acc did not improve\n",
      "50/50 [==============================] - 4s 75ms/step - loss: 22.3297 - acc: 0.9788 - val_loss: 23.1728 - val_acc: 0.6452\n",
      "Epoch 20/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 21.6806 - acc: 0.9898\n",
      "Epoch 00020: val_acc did not improve\n",
      "50/50 [==============================] - 4s 73ms/step - loss: 21.6742 - acc: 0.9900 - val_loss: 22.4709 - val_acc: 0.6452\n",
      "Epoch 21/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 21.0318 - acc: 0.9936\n",
      "Epoch 00021: val_acc did not improve\n",
      "50/50 [==============================] - 4s 76ms/step - loss: 21.0251 - acc: 0.9938 - val_loss: 21.7810 - val_acc: 0.6452\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f179888d160>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit\n",
    "model.fit_generator(generator=my_generator(Xtrain, Ytrain, \n",
    "                                        length=FLAG.length, \n",
    "                                        n_channel=FLAG.n_channel, \n",
    "                                        n_classes=n_classes,\n",
    "                                        random_noise=FLAG.random_noise,\n",
    "                                        normalized=FLAG.normalized,\n",
    "                                        batch_size=FLAG.batch_size,\n",
    "                                        aug_func=augset,\n",
    "                                        prob=0.25),\n",
    "                    class_weight=class_weight,\n",
    "                    validation_data=(Xtest, Ytest, Wtest),\n",
    "                    steps_per_epoch=50, \n",
    "                    epochs=FLAG.epoch,\n",
    "                    verbose=1,\n",
    "                    callbacks=[csv_logger,\n",
    "                            reduce_lr, \n",
    "                            checkpoint,\n",
    "                            earlystop\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min val_loss 21.780989246983687 at epoch 20\n",
      "max val_accu 0.6774193548387096 at epoch 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:52: FutureWarning: 'argmin' is deprecated, use 'idxmin' instead. The behavior of 'argmin'\n",
      "will be corrected to return the positional minimum in the future.\n",
      "Use 'series.values.argmin' to get the position of the minimum now.\n",
      "  return getattr(obj, method)(*args, **kwds)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:52: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'\n",
      "will be corrected to return the positional maximum in the future.\n",
      "Use 'series.values.argmax' to get the position of the maximum now.\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# plot csv logger\n",
    "myutils.plot_keras_csv_logger(csv_logger, save_dir=model_save, accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate validation set\n",
    "trained_model = load_model(os.path.join(model_save,'model.h5'))\n",
    "Pred = trained_model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate by every segment\n",
    "ypred_aug = np.argmax(Pred , axis=1)\n",
    "ytest_aug = np.argmax(Ytest, axis=1)\n",
    "\n",
    "cfm = confusion_matrix(y_pred=ypred_aug, y_true=ytest_aug)\n",
    "\n",
    "plt.figure()\n",
    "myutils.plot_confusion_matrix(cfm, classes=np.arange(n_classes), title='Confusion matrix, without normalization')\n",
    "plt.savefig(os.path.join(model_save, 'segment_confusion_matrix.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# aggregate by voting\n",
    "#ypred = np.ceil(np.mean(ypred_aug.reshape(FLAG.k_slice,-1), axis=0))\n",
    "ypred = np.round(np.mean(ypred_aug.reshape(FLAG.k_slice,-1), axis=0), 0)\n",
    "#ypred = (np.mean(ypred_aug.reshape(FLAG.k_slice,-1), axis=0) > 0.5) + 0 # voting\n",
    "ytest = np.argmax(Yvalid, axis=1)\n",
    "\n",
    "\n",
    "# calculate aggregated results\n",
    "cfm = confusion_matrix(y_pred=ypred, y_true=ytest)\n",
    "recall = np.diag(cfm) / np.sum(cfm, axis=1)\n",
    "precision = np.diag(cfm) / np.sum(cfm, axis=0)\n",
    "vote_val_accu = accuracy_score(y_pred=ypred, y_true=ytest)\n",
    "\n",
    "\n",
    "for i in range(n_classes):\n",
    "    print('recall-{0} : {1}'.format(i, recall[i]))\n",
    "    #sav['precision-{0}'.format(i)] = precision[i]\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "myutils.plot_confusion_matrix(cfm, classes=np.arange(n_classes), title='Confusion matrix, without normalization')\n",
    "plt.savefig(os.path.join(model_save, 'voting_confusion_matrix.png'))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# calculate average accuracy from segments\n",
    "# and voting accuracy\n",
    "tmp = ypred_aug.reshape(FLAG.k_slice,-1)\n",
    "savg_val_accu = 0.0\n",
    "for i in range(tmp.shape[0]):\n",
    "    accu = accuracy_score(y_pred=tmp[i,:], y_true=ytest)\n",
    "    print('{0}-segment accuracy={1}'.format(i, accu))\n",
    "    savg_val_accu += accu\n",
    "savg_val_accu /= tmp.shape[0]\n",
    "print('avg accu={0}'.format(savg_val_accu))\n",
    "print('vote accu={0}'.format(vote_val_accu))\n",
    "\n",
    "class_ratio = collections.Counter(ytest)\n",
    "for i in range(len(class_ratio)):\n",
    "    print('Ytest ratio class-%s: %s' % (i, class_ratio[i]/len(ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read traing.log\n",
    "loss = pd.read_table(csv_logger.filename, delimiter=',')\n",
    "best_val_loss = np.min(loss.val_loss)\n",
    "best_epoch = np.argmin(loss.val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save into dictionary\n",
    "sav = vars(FLAG)\n",
    "sav['epoch'] = best_epoch\n",
    "sav['val_loss'] = best_val_loss\n",
    "sav['vote_val_accu'] = vote_val_accu\n",
    "sav['savg_val_accu'] = savg_val_accu\n",
    "sav['model_id'] = model_id\n",
    "\n",
    "for i in range(n_classes):\n",
    "    sav['recall-{0}'.format(i)] = recall[i]\n",
    "    sav['precision-{0}'.format(i)] = precision[i]\n",
    "\n",
    "# append into summary files\n",
    "dnew = pd.DataFrame(sav, index=[0])\n",
    "if os.path.exists(summary_save):\n",
    "    dori = pd.read_csv(summary_save)\n",
    "    dori = pd.concat([dori, dnew])\n",
    "    dori.to_csv(summary_save, index=False)\n",
    "else:\n",
    "    dnew.to_csv(summary_save, index=False)\n",
    "\n",
    "print(summary_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
